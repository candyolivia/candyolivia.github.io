<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Candy Olivia Mawalim | Speech Science and Technology in Hearing Aids </title>
  <meta name="description" content="Personal website of Candy Olivia Mawalim.">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/teaching/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Candy Olivia</span> Mawalim</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
            </a>
          </li>
          
          <li class="nav-item ">
            <a class="nav-link" href="/profile/">
            Profile
            </a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="/teaching/">
            Teaching  
            </a>
          </li>
          
          <li class="nav-item navbar-active font-weight-bold">
            <a class="nav-link" href="/research/">
            Research
            <span class="sr-only">(current)</span>
            </a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="/publications/">
            Publications
            </a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="/news/">
            News  
            </a>
          </li>
          
          <li class="nav-item ">
            <a class="nav-link" href="/contact/">
              Contact
            </a>
          </li>

        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
    <h1>Speech Science and Technology in Hearing Aids</h1>

    <p><br /></p>
    
    <div class="clearfix" align="center">
      <img width="60%" src="/assets/img/research/hearing_aids/hearing_impaired.png">
    </div>

    <p><br /></p>

    <div class="text-justify p-0">
      <div class="clearfix" align="justify">

        <p>Hearing loss is a prevalent issue that affects a significant portion of the population. The statistics reveal that one in six people experience hearing loss, and this number is expected to rise significantly by 2030. The aging population is particularly vulnerable to hearing loss, with one in four people over 65 years old experiencing it. However, the younger generation is also at risk due to exposure to excessive noise, which is becoming increasingly common in our daily lives. With the advent of new technologies and the prevalence of portable music players, young people are exposed to loud music for extended periods, which can cause permanent damage to their hearing.</p>

        <p>The consequences of hearing loss are significant, as it is linked to several adverse health outcomes. Hearing loss can lead to social isolation, loneliness, and depression. It can also affect cognitive function and lead to dementia, particularly in older adults. The impact of hearing loss on quality of life is profound, with individuals experiencing communication difficulties, reduced job opportunities, and diminished overall well-being. The economic impact of hearing loss is also significant, with estimates suggesting that it costs billions of dollars annually in lost productivity and healthcare costs.</p>

        <p>To address the issue of hearing loss, hearing aids are commonly used to assist individuals with hearing loss. However, hearing aids have limitations, particularly in noisy environments. Many individuals with hearing loss find it challenging to communicate in noisy situations, such as restaurants, public transport, or social gatherings. The inability to hear adequately in these situations can lead to frustration and further isolation. Therefore, improving the performance of hearing aids in noisy environments is crucial to enable individuals with hearing loss to participate fully in society and improve their quality of life.</p>

      </div>
    </div>

    <p><br /></p>

    <div class="card mt-3 p-3">
      <h3 class="card-title"><b>Related Publications</b></h3>
      <hr>
      <div>
        <ol class="bibliography">
           <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://www.ihmmsec.org/cms/home/home2025.html" target="_blank">
                  WASPAA
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="ZMU2025_waspaa" class="col p-0">
                  <h5 class="title mb-0">Modeling Multi-Level Hearing Loss for Speech Intelligibility Prediction</h5>
                  <div class="author">
                     <nobr>Xiajie Zhou</nobr>, <nobr><em>Candy Olivia Mawalim</em></nobr>, <nobr>Masashi Unoki</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA 2025), Granlibakken Tahoe, Tahoe City, CA, 12-15 October 2025.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#ZMU2025_waspaa-abstract" role="button" aria-expanded="false" aria-controls="ZMU2025_waspaa-abstract">Abstract</a>
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/CPC3_E020a_E020b_Final.pdf" target="_blank">PDF</a> -->
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://dl.acm.org/doi/10.1145/3733102.3736706" target="_blank">URL</a> -->
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="ZMU2025_waspaa-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        The diverse perceptual consequences of hearing loss severely impede speech communication, but standard clinical audiometry, which is focused on threshold-based frequency sensitivity, does not adequately capture deficits in frequency and temporal resolution. To address this limitation, we propose a speech intelligibility prediction method that explicitly simulates auditory degradations according to hearing loss severity by broadening cochlear filters and applying low-pass modulation filtering to temporal envelopes. Speech signals are subsequently analyzed using the spectro-temporal modulation (STM) representations, which reflect how auditory resolution loss alters the underlying modulation structure. In addition, normalized cross-correlation (NCC) matrices quantify the similarity between the STM representations of clean speech and speech in noise. These auditory-informed features are utilized to train a Vision Transformer–based regression model that integrates the STM maps and NCC embeddings to estimate speech intelligibility scores. Evaluations on the Clarity Prediction Challenge corpus show that the proposed method outperforms the hearing-aid speech perception index (HASPI) in both mild and moderate-to-severe hearing loss groups, with a relative root mean squared error reduction of 16.5% for the mild group and a 6.1% reduction for the moderate-to-severe group. These results highlight the importance of explicitly modeling listener-specific frequency and temporal resolution degradations to improve speech intelligibility prediction and provide interpretability in auditory distortions.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>


          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://www.ihmmsec.org/cms/home/home2025.html" target="_blank">
                  Clarity
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="MZU2025_clarity" class="col p-0">
                  <h5 class="title mb-0">Integrating Linguistic and Acoustic Cues for Machine Learning-Based Speech Intelligibility Prediction in Hearing Impairment</h5>
                  <div class="author">
                     <nobr><em>Candy Olivia Mawalim</em></nobr>, <nobr>Xiajie Zhou</nobr>, <nobr>Huy Quoc Nguyen</nobr>, <nobr>Masashi Unoki</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The 6th Clarity Workshop on Improving Speech-in-Noise for Hearing Devices (Clarity-2025), Delft, The Netherlands, 22nd August 2025.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#MZU2025_clarity-abstract" role="button" aria-expanded="false" aria-controls="MZU2025_clarity-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/CPC3_E020c_final.pdf" target="_blank">PDF</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/poster/Clarity2025_E020c_Poster_R3.pdf" target="_blank">Poster</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="MZU2025_clarity-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Speech intelligibility prediction for individuals with hearing loss is paramount for advancing hearing aid technology. Leveraging recent breakthroughs in ASR foundation models, particularly Whisper, we fine-tuned a Whisper model for speech intelligibility prediction. Our approach incorporates data augmentation using impulse responses from diverse everyday environments. This study investigates the effective integration of linguistic and acoustic cues to enhance the prediction of finetune ASR models, aiming to compensate for both hearing loss and information loss during signal downsampling. Our goal is to improve speech intelligibility prediction, especially in noisy conditions. Experiments demonstrate that integrating these cues is beneficial. Furthermore, employing a weighted average ensemble model, which balances predictions from left and right audio channels and considers both stable and unstable linguistic and acoustic cues, significantly improved prediction performance, reducing the RMSE by approximately 2 and enhancing the Pearson correlation coefficient (ρ) by around 0.05.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://www.ihmmsec.org/cms/home/home2025.html" target="_blank">
                  Clarity
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="ZMU2025_clarity" class="col p-0">
                  <h5 class="title mb-0">Lightweight Speech Intelligibility Prediction with Spectro-Temporal Modulation for Hearing-Impaired Listeners</h5>
                  <div class="author">
                     <nobr>Xiajie Zhou</nobr>, <nobr><em>Candy Olivia Mawalim</em></nobr>, <nobr>Huy Quoc Nguyen</nobr>, <nobr>Masashi Unoki</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The 6th Clarity Workshop on Improving Speech-in-Noise for Hearing Devices (Clarity-2025), Delft, The Netherlands, 22nd August 2025.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#ZMU2025_clarity-abstract" role="button" aria-expanded="false" aria-controls="ZMU2025_clarity-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/CPC3_E020a_E020b_Final.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="ZMU2025_clarity-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Hearing loss leads to reduced frequency resolution and impaired temporal resolution, making it difficult for listeners to distinguish similar sounds and perceive speech dynamics in noise. To capture these perceptual degradations, we employ spectrotemporal modulation (STM) analysis as the core feature representation. This study proposes a speech intelligibility prediction framework that uses STM representations as input to lightweight convolutional neural network (CNN) models. We design two models: STM-CNN-SE (E020a), which incorporates squeeze-and-excitation (SE) block, and STM-CNN-ECA (E020b), which uses efficient channel attention (ECA) block and richer input features. Compared to the HASPI, experiments on the CPC3 development dataset show that E020a and E020b reduce root-mean-square error (RMSE) by 11.2% and 12.6%, respectively. These results demonstrate the effectiveness of STM-based CNN architectures for speech intelligibility prediction under hearing loss conditions.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="https://ieeeaccess.ieee.org/" target="_blank">
                  IEEE Access
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="zmu_access2025" class="col p-0">
                  <h5 class="title mb-0">Speech Intelligibility Prediction Using Binaural Processing for Hearing Loss</h5>
                  <div class="author">
                    <nobr>Xiajie Zhou,</nobr> <nobr><em>Candy Olivia Mawalim</em>,</nobr> and
                    <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      IEEE Access, Jan 2025.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#zmu_access2025-abstract" role="button" aria-expanded="false" aria-controls="zmu_access2025-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/ACCESS.2025.3538708" target="_blank">DOI</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/Access2025_Xiajie.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="zmu_access2025-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        As the global issue of hearing loss becomes increasingly severe, developing effective speech intelligibility prediction methods is crucial for improving the performance of hearing aids. However, current methods struggle in noisy environments and overlook individual differences in hearing loss between ears, which impacts prediction accuracy. Therefore, this study proposes a non-intrusive speech intelligibility prediction method that incorporates the binaural processing for hearing loss. The proposed method simulates the multi-stage binaural processing of the outer, middle, and inner ear and integrates binaural cues through an equalization-cancellation model to mitigate masking effects in noisy environments. Key features extracted from speech signals serve as inputs for a hybrid speech intelligibility model combining long short-term memory (LSTM) and light gradient boosting machine (LightGBM) models. The proposed method captures the critical features of speech signals, especially in challenging environments and for different types of hearing loss. Experimental results show that, compared to the baseline system of the second Clarity Prediction Challenge (CPC2) dataset, the proposed method achieves an 8.3% reduction in root mean squared error (RMSE). Notably, the proposed method reduces RMSE by 12.8% when predicting inconsistent hearing loss compared to listeners with consistent hearing levels, confirming the potential of combining hearing loss modeling with binaural processing.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://interspeech2024.org/" target="_blank">
                  Interspeech
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="mou2024_interspeech" class="col p-0">
                  <h5 class="title mb-0">Are Recent Deep Learning-Based Speech Enhancement Methods Ready to Confront Real-World Noisy Environments?</h5>
                  <div class="author">
                     <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Shogo Okada,</nobr> 
                    and <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The 25th Interspeech Conference, Kos Island, Greece, 1-5 September 2024.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mou2024_interspeech-abstract" role="button" aria-expanded="false" aria-controls="mou2024_interspeech-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/mawalim24_interspeech.pdf" target="_blank">PDF</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.isca-archive.org/interspeech_2024/mawalim24_interspeech.html#" target="_blank">DOI</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/candyolivia/is2024_deep_enhancement" target="_blank">Code</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://candyolivia.github.io/demo/is2024_enhancement/" target="_blank">Demo</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="mou2024_interspeech-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Recent advancements in speech enhancement techniques have ignited interest in improving speech quality and intelligibility. However, the effectiveness of recently proposed methods is unclear. In this paper, a comprehensive analysis of modern deep learning-based speech enhancement approaches is presented. Through evaluations using the Deep Suppression Noise and Clarity Enhancement Challenge datasets, we assess the performances of three methods: Denoiser, DeepFilterNet3, and FullSubNet+. Our findings reveal nuanced performance differences among these methods, with varying efficacy across datasets. While objective metrics offer valuable insights, they struggle to represent complex scenarios with multiple noise sources. Leveraging ASR-based methods for these scenarios shows promise but may induce critical hallucination effects. Our study emphasizes the need for ongoing research to refine techniques for diverse real-world environments.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://www.apsipa2023.org/" target="_blank">
                  APSIPA
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="zmu2023_apsipa" class="col p-0">
                  <h5 class="title mb-0">Incorporating the Digit Triplet Test in A Lightweight Speech Intelligibility Prediction for Hearing Aids.</h5>
                  <div class="author">
                    <nobr>Xiajie Zhou,</nobr>
                    <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                    and
                    <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The 15th Asia-Pasific Signal and Information Processing Association (APSIPA ASC 2023), Taipei, Taiwan, 31 October - 3 November 2023.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#zmu2023_apsipa-abstract" role="button" aria-expanded="false" aria-controls="zmu2023_apsipa-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/APSIPAASC58517.2023.10317260" target="_blank">DOI</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/10317260" target="_blank">IEEE</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/APSIPA2023_Zhou.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="zmu2023_apsipa-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Recent studies in speech processing often utilize sophisticated methods for solving a task to obtain high-accuracy results. Although high performance could be achieved, the methods are too complex and require high-performance computational power that might not be available for a wide range of researchers. In this study, we propose a method to incorporate the low dimensional and the recent state-of-the-art acoustic features for speech processing to predict the speech intelligibility in noise for hearing aids. The proposed method was developed based on the stack regressor on various traditional machine learning regressors. Unlike other existing works, we utilized the results of the digit triplet test, which is usually used to measure the hearing ability in the existence of noise, to improve the prediction. The evaluation of our proposed method was carried out by using the first Clarity Prediction Challenge dataset. This dataset is utilized for speech intelligibility prediction that consists of speech signals output of hearing aids that were arranged in various simulated scenes with interferers. Our experimental results show that the proposed method could improve speech intelligibility prediction. The results also show that the digit triplet test results are beneficial for speech intelligibility prediction in noise.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="https://www.sciencedirect.com/journal/applied-acoustics" target="_blank">
                  APAC
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="mtou_apac2023" class="col p-0">
                  <h5 class="title mb-0">Non-Intrusive Speech Intelligibility Prediction Using an Auditory Periphery Model with Hearing Loss.</h5>
                  <div class="author">
                    <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Benita Angela Titalim,</nobr> <nobr>Shogo Okada,</nobr> 
                    and
                    <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      Applied Acoustics, 2023.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mtou_apac2023-abstract" role="button" aria-expanded="false" aria-controls="mtou_apac2023-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1016/j.apacoust.2023.109663" target="_blank">DOI</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/1-s2.0-S0003682X23004619-main.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="mtou_apac2023-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Speech intelligibility prediction methods are necessary for hearing aid development. However, many such prediction methods are categorized as intrusive metrics because they require reference speech as input, which is often unavailable in real-world situations. Additionally, the processing techniques in hearing aids may cause temporal or frequency shifts, which degrade the accuracy of intrusive speech intelligibility metrics. This paper proposes a non-intrusive auditory model for predicting speech intelligibility under hearing loss conditions. The proposed method requires binaural signals from hearing aids and audiograms representing the hearing conditions of hearing-impaired listeners. It also includes additional acoustic features to improve the method’s robustness in noisy and reverberant environments. A two-dimensional convolutional neural network with neural decision forests is used to construct a speech intelligibility prediction model. An evaluation conducted with the first Clarity Prediction Challenge dataset shows that the proposed method performs better than the baseline system.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>


          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://eusipco2023.org/" target="_blank">
                  EUSIPCO
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="mtou2023_eusipco" class="col p-0">
                  <h5 class="title mb-0">Auditory Model Optimization with Wavegram-CNN and Acoustic Parameter Models for Nonintrusive Speech Intelligibility Prediction in Hearing Aids.</h5>
                  <div class="author">
                    <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Benita Angela Titalim,</nobr> <nobr>Shogo Okada,</nobr>
                    and
                    <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The 31st European Signal Processing Conference (EUSIPCO 2023), Helsinki, Finland.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mtou2023_eusipco-abstract" role="button" aria-expanded="false" aria-controls="mtou2023_eusipco-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.23919/EUSIPCO58844.2023.10289742" target="_blank">DOI</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/EUSIPCO2023_Mawalim.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="mtou2023_eusipco-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Nonintrusive speech intelligibility (SI) prediction is essential for evaluating many speech technology applications, including hearing aid development. In this study, several factors related to hearing perception are investigated to predict SI. In the proposed method, we integrated a physiological auditory model from two ears (binaural EarModel), wavegram-CNN model and acoustic parameter model. The refined EarModel does not require clean speech as input (blind method). In EarModel, the perception caused by hearing loss is simulated based on audiograms. Meanwhile, the wavegram-CNN and acoustic parameter models represent the factors related to the speech spectrum and acoustics, respectively. The proposed method is evaluated based on the scenario from the 1st Clarity Prediction Challenge (CPC1). The results show that the proposed method outperforms the intrusive baseline MBSTOI and HASPI methods in terms of the Pearson coefficient (ρ), RMSE, and R2 score in both closed-set and open-set tracks. Based on the results from listener-wise evaluation results, the average $\rho$ could be improved by more than 0.3 using the proposed method.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>
          
          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://sst2022.com/" target="_blank">
                  SST
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="mtuo2022_sst" class="col p-0">
                  <h5 class="title mb-0">OBISHI: Objective Binaural Intelligibility Score for the Hearing Impaired.</h5>
                  <div class="author">
                    <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Benita Angela Titalim,</nobr> <nobr>Masashi Unoki, </nobr>
                    and
                    <nobr>Shogo Okada.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      SST2022, The 18th Australasian International Conference on Speech Science and Technology, Canberra, Australia, 13--16 December 2022.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mtuo2022_sst-abstract" role="button" aria-expanded="false" aria-controls="mtuo2022_sst-abstract">Abstract</a>
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.isca-speech.org/archive/interspeech_2020/mawalim20_interspeech.html" target="_blank">DOI</a> -->
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://sst2022.files.wordpress.com/2022/12/mawalim-et-al-2022-obishi-objective-binaural-intelligibility-score-for-the-hearing-impaired.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="mtuo2022_sst-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Speech intelligibility prediction for both normal hearing and hearing impairment is very important for hearing aid development. The Clarity Prediction Challenge 2022 (CPC1) was initiated to evaluate the speech intelligibility of speech signals produced by hearing aid systems. Modified binaural short-time objective intelligibility (MBSTOI) and hearing aid speech prediction index (HASPI) were introduced in the CPC1 to understand the basis of speech intelligibility prediction. This paper proposes a method to predict speech intelligibility scores, namely OBISHI. OBISHI is an intrusive (non-blind) objective measurement that receives binaural speech input and considers the hearing-impaired characteristics. In addition, a pre-trained automatic speech recognition (ASR) system was also utilized to infer the difficulty of utterances regardless of the hearing loss condition. We also integrated the hearing loss model by the Cambridge auditory group and the Gammatone Filterbank-based prediction model. The total evaluation was conducted by comparing the predicted intelligibility score of the baseline MBSTOI and HASPI with the actual correctness of listening tests. In general, the results showed that the proposed method, OBISHI, outperformed the baseline MBSTOI and HASPI (improved approximately 10% classification accuracy in terms of F1 score).
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <hr>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://www.apsipa2022.org/" target="_blank">
                  APSIPA
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="tmou2022_apsipa" class="col p-0">
                  <h5 class="title mb-0">Speech Intelligibility Prediction for Hearing Aids Using an Auditory Model and Acoustic Parameters.</h5>
                  <div class="author">
                    <nobr>Benita Angela Titalim*,</nobr>
                    <nobr><em>Candy Olivia Mawalim*</em>,</nobr>
                    <nobr>Shogo Okada,</nobr>
                    and
                    <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Chiang Mai, Thailand, 7--10 November 2022.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#tmou2022_apsipa-abstract" role="button" aria-expanded="false" aria-controls="tmou2022_apsipa-abstract">Abstract</a>
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/9689554" target="_blank">IEEE</a>
                     -->
                    <a class="badge grey waves-effect font-weight-light mr-1" href="http://www.apsipa.org/proceedings/2022/APSIPA%202022/WedPM2-2/1570834765.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="tmou2022_apsipa-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Objective speech intelligibility (SI) metrics for hearing-impaired people play an important role in hearing aid development. The work on improving SI prediction also became the basis of the first Clarity Prediction Challenge (CPC1). This study investigates a physiological auditory model called EarModel and acoustic parameters for SI prediction. EarModel is utilized because it provides advantages in estimating human hearing, both normal and impaired. The hearing-impaired condition is simulated in EarModel based on audiograms; thus, the SI perceived by hearing-impaired people is more accurately predicted. Moreover, the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) and WavLM, as additional acoustic parameters for estimating the difficulty levels of given utterances, are included to achieve improved prediction accuracy. The proposed method is evaluated on the CPC1 database. The results show that the proposed method improves the SI prediction effects of the baseline and hearing aid speech prediction index (HASPI). Additionally, an ablation test shows that incorporating the eGeMAPS and WavLM can significantly contribute to the prediction model by increasing the Pearson correlation coefficient by more than 15% and decreasing the root-mean-square error (RMSE) by more than 10.00 in both closed-set and open-set tracks.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          

        </ol>
      </div>
    </div>

    <p><br /></p>

  </div>

</div>

  <!-- Footer -->
  <footer>
    &copy; Copyright 2022 Candy Olivia Mawalim. Powered by <a href="http://jekyllrb.com/" target="_blank"><font color="#a20d0d">Jekyll</font></a> with <a href="https://github.com/alshedivat/al-folio"><font color="#a20d0d">al-folio</font></a> theme. Hosted by <a href="https://pages.github.com/" target="_blank"><font color="#a20d0d">GitHub Pages</font></a>.
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-54519238-1', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
