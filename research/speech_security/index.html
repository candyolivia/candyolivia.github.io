<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Candy Olivia Mawalim | Secure Speech Communication </title>
  <meta name="description" content="Personal website of Candy Olivia Mawalim.">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/teaching/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Candy Olivia</span> Mawalim</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
            </a>
          </li>
          
          <li class="nav-item ">
            <a class="nav-link" href="/profile/">
            Profile
            </a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="/teaching/">
            Teaching  
            </a>
          </li>
          
          <li class="nav-item navbar-active font-weight-bold">
            <a class="nav-link" href="/research/">
            Research
            <span class="sr-only">(current)</span>
            </a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="/publications/">
            Publications
            </a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="/news/">
            News  
            </a>
          </li>
          
          <li class="nav-item ">
            <a class="nav-link" href="/contact/">
              Contact
            </a>
          </li>

        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
    <h1>Secure Speech Communication</h1>

    <p><br /></p>
    
    <div class="clearfix" align="center">

      <iframe width="560" height="315" src="https://www.youtube.com/embed/GZlZFXaIcso" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </div>

    <p><br /></p>

    <div class="text-justify p-0">
      <div class="clearfix" align="justify">

        <p>In the digital age, where information is readily available, the threat of fake or manipulated content is becoming increasingly concerning. Experts predict that in the next six years, more than 90% of all digital content will have been manipulated to some degree. This alarming statistic highlights the need for effective solutions to combat the rising threat of fake information. Manipulated content has the potential to cause severe damage, whether it is used for propaganda, disinformation, or cybercrime. The implications of this are far-reaching, with the potential to impact public opinion, disrupt democracy, and damage reputations.</p>

        <p>Despite advancements in detection techniques, current technology is only able to detect about 65% of fake information, with the remaining 35% slipping through the internet undetected. This represents a significant challenge, as the technology required to create and distribute fake information is becoming increasingly accessible. The ability to create high-quality, convincing fake information with ease is a growing concern, and as technology continues to evolve, the challenge of detecting fake information will become even greater. This is particularly true when it comes to speech signals, which can be manipulated to create fake audio recordings or deepfakes that are difficult to distinguish from genuine recordings.</p>

        <p>Given the potential harm that can be caused by fake information, it is crucial to develop effective solutions to combat this threat. This research aims to propose solutions specifically for the protection and detection of manipulated speech signals, which are increasingly being used to spread fake information. Detecting fake speech signals presents unique challenges, as these signals can be highly complex, making them difficult to detect using traditional methods. Furthermore, as deepfake technology improves, the challenge of detecting fake speech signals is only likely to increase. This research aims to address these challenges by proposing innovative solutions that can detect and prevent the spread of fake speech signals, ultimately contributing to the development of a safer digital landscape.</p>

      </div>
    </div>

    <p><br /></p>

    <div class="card mt-3 p-3">
      <h3 class="card-title"><b>Related Publications</b></h3>
      <hr>
      <div>
        <ol class="bibliography">

          
          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://2025.ieeeicassp.org/" target="_blank">
                  ICASSP
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="mau2025_icassp" class="col p-0">
                  <h5 class="title mb-0">Fine-tuning TitaNet-Large Model for Speaker Anonymization Attacker Systems.</h5>
                  <div class="author">
                     <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Aulia Adila,</nobr> and <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The 2025 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2025), Hyderabad, India (Accepted).
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mau2025_icassp-abstract" role="button" aria-expanded="false" aria-controls="mau2025_icassp-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/mau2025_icassp.pdf" target="_blank">PDF</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/ICASSP49660.2025.10888822" target="_blank">DOI</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="mau2025_icassp-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Speaker anonymization techniques are crucial for safeguarding user privacy in voice-based applications. However, these methods are susceptible to adversarial attacks that can compromise their effectiveness. This paper proposes attacker systems that leverage the power of fine-tuned TitaNet-Large and ECAPA-TDNN models to identify the original speaker from anonymized speech generated by various anonymization methods. Both pre-trained models are renowned for their stateof-the-art ability to extract robust speaker embeddings. Finetuning these models with anonymized speech enables them to identify underlying patterns in anonymized speech. We evaluated the proposed attacker systems against multiple anonymization techniques that performed effectively in a series of voice privacy challenges. Our experimental results underscore the effectiveness of the fine-tuned TitaNet-Large model in breaking through these anonymization methods, as indicated by the reduced equal error rate (EER). This highlights the importance of robust and adaptive anonymization strategies to counter such emerging semiinformed threats.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>


          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://sealp-workshop.github.io/" target="_blank">
                  SEALP
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="alpmsu2025_sealp" class="col p-0">
                  <h5 class="title mb-0">Indonesian Speech Content De-Identification in Low Resource Transcripts.</h5>
                  <div class="author">
                     <nobr>Rifqi Naufal Abdjul, </nobr><nobr>Dessi Puji Lestari, </nobr><nobr>Ayu Purwarianti, </nobr> 
                     <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Sakriani Sakti,</nobr> and <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The Second Workshop in South East Asian Language Processing, Co-located with COLING 2025, Abu Dhabi (Online), January 19-24, 2025.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#alpmsu2025_sealp-abstract" role="button" aria-expanded="false" aria-controls="alpmsu2025_sealp-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/alpmsu2025_sealp.pdf" target="_blank">PDF</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/2025.sealp-1.6/" target="_blank">URL</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="alpmsu2025_sealp-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Advancements in technology and the increased use of digital data threaten individual privacy, especially in speech containing Personally Identifiable Information (PII). Therefore, systems that can remove or process privacy-sensitive data in speech are needed, particularly for low-resource transcripts. These transcripts are minimally annotated or labeled automatically, which is less precise than human annotation. However, using them can simplify the development of de-identification systems in any language. In this study, we develop and evaluate an efficient speech de-identification system. We create an Indonesian speech dataset containing sensitive private information and design a system with three main components: speech recognition, information extraction, and masking. To enhance performance in low-resource settings, we incorporate transcription data in training, use data augmentation, and apply weakly supervised learning. Our results show that our techniques significantly improve privacy detection performance, with approximately 29% increase in F1 score, 20% in precision, and 30% in recall with minimally labeled data.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

         
          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="https://aes2.org/publications/journal/" target="_blank">
                  JAES
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="fammwpkcp_jaes2024" class="col p-0">
                  <h5 class="title mb-0">Forensic speech enhancement: Toward Reliable Handling of Poor-Quality Speech Recordings Used as Evidence in Criminal Trials</h5>
                  <div class="author"><nobr>Helen Fraser,</nobr> <nobr>Vincent Aubanel,</nobr> <nobr>Robert C. Maher,</nobr> <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Xin Wang,</nobr> <nobr>Peter Počta,</nobr> <nobr>Emma Keith,</nobr> <nobr>Gérard Chollet,</nobr> 
                    and
                    <nobr>Karla Pizzi.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      Journal of the Audio Engineering Society, 2024 (Accepted).
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#fammwpkcp_jaes2024-abstract" role="button" aria-expanded="false" aria-controls="fammwpkcp_jaes2024-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.17743/jaes.2022.0176" target="_blank">DOI</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/Forensic_Speech_Enhancement__Toward_Reliable_Handling_of_Poor-Quality_Speech_Recordings_Used_as_Evidence_in_Criminal_Trials__doc.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="fammwpkcp_jaes2024-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        This paper proposes an innovative interdisciplinary approach to evaluating the effectiveness of forensic speech enhancement (FSE). FSE faces unique challenges arising from a range of factors, from poor recording quality, highly variable conditions from case to case, and content uncertainty. Despite these difficulties, FSE is commonly admitted in court, and can significantly influence the outcome of criminal trials. Current FSE practices are hindered by unrealistic expectations from courts, which often assume that enhanced audio inherently clarifies content. In fact, FSE can have the undesired opposite effect, potentially resulting in unfair prejudice, when, for example, it increases the credibility of a misleading transcript. The proposed interdisciplinary project advocates for a better consideration of speech perception factors, particularly those related to transcription. It aims to bridge the gap between FSE and forensic transcription by promoting a combined approach to enhancing and accurately transcribing forensic audio. By developing a position statement on FSE capabilities, the project seeks to establish realistic standards and foster collaboration among researchers and practitioners. This effort aims to ensure reliable, accountable forensic audio evidence, aligning with forensic science standards and improving the effectiveness of the justice system.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="http://www.apsipa2024.org/" target="_blank">
                  APSIPA
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="amu_apsipa2024" class="col p-0">
                  <h5 class="title mb-0">Detecting Spoof Voices in Asian Non-Native Speech: An Indonesian and Thai Case Study.</h5>
                  <div class="author">
                     <nobr>Aulia Adila,</nobr> <nobr><em>Candy Olivia Mawalim</em>,</nobr> and <nobr>Masashi Unoki</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The 16th annual conference organized by Asia-Pacific Signal and Information Processing Association (APSIPA 2024), Galaxy International Convention Center, Macau, China, 3-6 Dec 2024. (Accepted)
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#amu_apsipa2024-abstract" role="button" aria-expanded="false" aria-controls="amu_apsipa2024-abstract">Abstract</a>
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/NCSP_AMIU24.pdf" target="_blank">PDF</a> -->
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">DOI</a> -->
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">Code</a> -->
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">Demo</a> -->
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="amu_apsipa2024-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        TBD
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://ococosda2024.github.io/" target="_blank">
                  O-COCOSDA
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="anhpgm_ococosda2024" class="col p-0">
                  <h5 class="title mb-0">Analysis of Pathological Features for Spoof Detection.</h5>
                  <div class="author">
                     <nobr>Myat Aye Aye Aung,</nobr> <nobr>Hay Mar Soe Naing,</nobr> <nobr>Aye Mya Hlaing,</nobr> <nobr>Win Pa Pa,</nobr> <nobr>Kasorn Galajit,</nobr> and <nobr><em>Candy Olivia Mawalim</em></nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The 27th International Conference of the Oriental COCOSDA (O-COCOSDA 2024), National Yang Ming Chiao Tung University, Hsinchu, Taiwan, 17-19 Oct 2024. (Accepted)
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#anhpgm_ococosda2024-abstract" role="button" aria-expanded="false" aria-controls="anhpgm_ococosda2024-abstract">Abstract</a>
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/NCSP_AMIU24.pdf" target="_blank">PDF</a> -->
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">DOI</a> -->
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">Code</a> -->
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">Demo</a> -->
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="anhpgm_ococosda2024-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        TBD
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://ococosda2024.github.io/" target="_blank">
                  O-COCOSDA
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="nphagm_ococosda2024" class="col p-0">
                  <h5 class="title mb-0">UCSYSpoof: A Myanmar Language Dataset for Voice Spoofing Detection.</h5>
                  <div class="author">
                     <nobr>Hay Mar Soe Naing,</nobr> <nobr>Win Pa Pa,</nobr> <nobr>Aye Mya Hlaing,</nobr> <nobr>Myat Aye Aye Aung,</nobr> <nobr>Kasorn Galajit,</nobr> and <nobr><em>Candy Olivia Mawalim</em></nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The 27th International Conference of the Oriental COCOSDA (O-COCOSDA 2024), National Yang Ming Chiao Tung University, Hsinchu, Taiwan, 17-19 Oct 2024. (Accepted)
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#nphagm_ococosda2024-abstract" role="button" aria-expanded="false" aria-controls="nphagm_ococosda2024-abstract">Abstract</a>
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/NCSP_AMIU24.pdf" target="_blank">PDF</a> -->
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">DOI</a> -->
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">Code</a> -->
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">Demo</a> -->
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="nphagm_ococosda2024-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        TBD
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://icaicta.cs.tut.ac.jp/2024/" target="_blank">
                  ICAICTA
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="aml_icaicta2024" class="col p-0">
                  <h5 class="title mb-0">Indonesian Speech Anti-Spoofing System: Data Creation and CNN Models.</h5>
                  <div class="author">
                     <nobr>Sarah Azka Arief,</nobr> <nobr><em>Candy Olivia Mawalim</em>,</nobr>  
                    and <nobr>Dessi Puji Lestari</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The 11th International Conference on Advanced Informatics: Concepts, Theory, and Applications (ICAICTA 2024), Singapore, 28-30 Sept 2024.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#aml_icaicta2024-abstract" role="button" aria-expanded="false" aria-controls="aml_icaicta2024-abstract">Abstract</a>
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/NCSP_AMIU24.pdf" target="_blank">PDF</a> -->
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">DOI</a> -->
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">Code</a> -->
                    <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">Demo</a> -->
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="aml_icaicta2024-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Biometric systems are prone to spoofing attacks. While research in speech anti-spoofing has been progressing, there is a limited availability of diverse language datasets. This study aims to bridge this gap by developing an Indonesian spoofed speech dataset, which includes replay attacks, text-to- speech, and voice conversion. This dataset forms the foundation for creating an Indonesian speech anti-spoofing system. Subsequently, light convolutional neural network (LCNN) and residual network (ResNet) models, based on convolutional neural networks (CNN), were developed to evaluate the dataset. The input features used are linear frequency cepstral coefficients (LFCC). Both models demonstrate remarkably low minDCF and EER scores approaching zero. The results also exhibit exceptional scores with 4-fold cross validation, showing strong initial performance with no signs of overfitting. However, models trained solely on Common Voice or Prosa.ai datasets performed poorly in cross-source tests, suggesting generalization issues due to a lack of diversity in the dataset. This highlights the need for further improvement and continued research in Indonesian speech spoof detection.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="https://www.jstage.jst.go.jp/browse/jsp/-char/en" target="_blank">
                  RISP
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="amiu_risp2024" class="col p-0">
                  <h5 class="title mb-0">Study on Inaudible Speech Watermarking Method Based on Spread-Spectrum Using Linear Prediction Residue.</h5>
                  <div class="author"><nobr>Aulia Adila,</nobr> <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Takuto Isoyama,</nobr> 
                    and
                    <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      Journal of Signal Processing, Research Institute of Signal Processing, 2024, Volume 28 Issue 6 Pages 309-313.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#amiu_risp2024-abstract" role="button" aria-expanded="false" aria-controls="amiu_risp2024-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.2299/jsp.28.309" target="_blank">DOI</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/risp_AMU2024_28_309.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="amiu_risp2024-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        A reliable speech watermarking technique must balance satisfying four requirements: inaudibility, robustness, blind detectability, and confidentiality. A previous study proposed a speech watermarking technique based on direct spread spectrum (DSS) using a linear prediction (LP) scheme, i.e., LP-DSS, that could simultaneously satisfy these four requirements. However, an inaudibility issue was found due to the incorporation of a blind detection scheme with frame synchronization. In this paper, we investigate the feasibility of utilizing a psychoacoustical model, which simulates auditory masking, to control the suitable embedding level of the watermark signal to resolve the inaudibility issue in the LP-DSS scheme. Evaluation results confirmed that controlling the embedding level with the psychoacoustical model, with a constant scaling factor setting, could balance the trade-off between inaudibility and detection ability with a payload up to 64 bps.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>


          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://isai-nlp-aiot2023.aiat.or.th/" target="_blank">
                  iSAI-NLP
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="ivo_isainlp2023" class="col p-0">
                  <h5 class="title mb-0">ThaiSpoof: A Database for Spoof Detection in Thai Language.</h5>
                  <div class="author">
                    <nobr>Kasorn Galajit,</nobr>
                    <nobr>Thunpisit Kosolsriwiwat,</nobr>
                    <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                    <nobr>Pakinee Aimmanee,</nobr>
                    <nobr>Waree Kongprawechnon,</nobr>
                    <nobr>Win Pa Pa,</nobr>
                    <nobr>Anuwat Chaiwongyen,</nobr>
                    <nobr>Teeradaj Racharak,</nobr>
                    <nobr>Hayati Yassin,</nobr>
                    <nobr>Jessada Karnjana,</nobr>
                    <nobr>Surasak Boonkla,</nobr>
                    and
                    <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The 18th International Joint Symposium on Artificial Intelligence and Natural Language Processing and The International Conference on Artificial Intelligence and Internet of Things (iSAI-NLP 2023).
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#ivo_isainlp2023-abstract" role="button" aria-expanded="false" aria-controls="ivo_isainlp2023-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">DOI</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/10354956" target="_blank">IEEE</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/2023287845_thaispoof.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="ivo_isainlp2023-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Many applications and security systems have widely applied automatic speaker verification (ASV). However, these systems are vulnerable to various direct and indirect access attacks, which weakens their authentication capability. The research in spoofed speech detection contributes to enhancing these systems. However, the study in spoofing detection is limited to only some languages due to the need for various datasets. This paper focuses on a Thai language dataset for spoof detection. The dataset consists of genuine speech signals and various types of spoofed speech signals. The spoofed speech dataset is generated using text-to-speech tools for the Thai language, synthesis tools, and tools for speech modification. To showcase the utilization of this dataset, we implement a simple model based on a convolutional neural network (CNN) taking linear frequency cepstral coefficients (LFCC) as its input. We trained, validated, and tested the model on our dataset referred to as ThaiSpoof. The experimental result shows that the accuracy of model is 95%, and equal error rate (EER) is 4.67%. The result shows that our ThaiSpoof dataset has the potential to develop for helping in spoof detection studies.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://isai-nlp-aiot2023.aiat.or.th/" target="_blank">
                  iSAI-NLP
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="mgmkia_isainlp2023" class="col p-0">
                  <h5 class="title mb-0">Voice Contribution on LFCC feature and ResNet-34 for Spoof Detection.</h5>
                  <div class="author">
                    <nobr>Khaing Zar Mon,</nobr>
                    <nobr>Kasorn Galajit,</nobr>
                    <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                    <nobr>Jessada Karnjana,</nobr>
                    <nobr>Tsuyoshi Isshiki,</nobr>
                    and
                    <nobr>Pakinee Aimmanee.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The 18th International Joint Symposium on Artificial Intelligence and Natural Language Processing and The International Conference on Artificial Intelligence and Internet of Things (iSAI-NLP 2023).
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mgmkia_isainlp2023-abstract" role="button" aria-expanded="false" aria-controls="mgmkia_isainlp2023-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354625" target="_blank">DOI</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10354625" target="_blank">IEEE</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/Spoof_Detection_using_Voice_Contribution_on_LFCC_features_and_ResNet-34.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="mgmkia_isainlp2023-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Recently, biometric authentication has been significant advancement, particularly in speaker verification. While there have been significant advancements in this technology, compelling evidence highlights the continued vulnerability of this technology to malicious spoofing attacks. This vulnerability calls for developing specialized countermeasures to identify various attack types. This paper specifically focuses on detecting replay, speech synthesis, and voice conversion attacks. As our spoof detection strategy’s front-end feature extraction method, we used linear frequency cepstral coefficients (LFCC). We utilized ResNet-34 to classify between genuine and fake speech. By integrating LFCC with ResNet-34, We evaluated the proposed method using the ASVspoof 2019 dataset, PA (Physical Access), and LA (Logical Access). In our approach, we compare the use of the entire utterance for feature extraction in both PA and LA datasets with an alternative method that involves extracting features from a specific percentage of the voice segment within the utterance for classification. In addition, we conducted a comprehensive evaluation by comparing our proposed method with the established baseline techniques, LFCC-GMM and CQCC-GMM. The proposed method demonstrates promising performance, achieving equal error rate (EER) of 3.11% and 3.49% for the development and evaluation datasets, respectively, in PA attacks. In LA attacks evaluation, the proposed method performs EER of 0.16% and 6.89% for the development and evaluation datasets, respectively. The proposed method shows promising results in identifying spoof attacks for both PA and LA attacks.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://www.apsipa2023.org/" target="_blank">
                  APSIPA
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="cmlwu2023_apsipa" class="col p-0">
                  <h5 class="title mb-0">Analysis of Spectro-Temporal Modulation Representation for Deep-Fake Speech Detection.</h5>
                  <div class="author">
                    <nobr>Haowei Cheng,</nobr>
                    <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                    <nobr>Kai Li,</nobr>
                    <nobr>Lijun Wang,</nobr>
                    and
                    <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      The 15th Asia-Pasific Signal and Information Processing Association (APSIPA ASC 2023), Taipei, Taiwan, 31 October - 3 November 2023.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#cmlwu2023_apsipa-abstract" role="button" aria-expanded="false" aria-controls="cmlwu2023_apsipa-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/APSIPAASC58517.2023.10317309" target="_blank">DOI</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/10317309" target="_blank">IEEE</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/APSIPA2023_Cheng.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="cmlwu2023_apsipa-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Deep-fake speech detection aims to develop effective techniques for identifying fake speech generated using advanced deep-learning methods. It can reduce the negative impact of malicious production or dissemination of fake speech in real-life scenarios. Although humans can relatively easy to distinguish between genuine and fake speech due to human auditory mechanisms, it is difficult for machines to distinguish them correctly. One major reason for this challenge is that machines struggle to effectively separate speech content from human vocal system information. Common features used in speech processing face difficulties in handling this issue, hindering the neural network from learning the discriminative differences between genuine and fake speech. To address this issue, we investigated spectro-temporal modulation representations in genuine and fake speech, which simulate the human auditory perception process. Next, the spectro-temporal modulation was fit to a light convolutional neural network bidirectional long short-term memory for classification. We conducted experiments on the benchmark datasets of the Automatic Speaker Verification and Spoofing Countermeasures Challenge 2019 (ASVspoof2019) and the Audio Deep synthesis Detection Challenge 2023 (ADD2023), achieving an equal-error rate of 8.33\% and 42.10\%, respectively. The results showed that spectro-temporal modulation representations could distinguish the genuine and deep-fake speech and have adequate performance in both datasets.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>


          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://www.apsipa2022.org/" target="_blank">
                  APSIPA
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="mou2022_apsipa" class="col p-0">
                  <h5 class="title mb-0">F0 Modification via PV-TSM Algorithm for Speaker Anonymization Across Gender.</h5>
                  <div class="author">
                    <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                    <nobr>Shogo Okada,</nobr>
                    and
                    <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Chiang Mai, Thailand, 7--10 November 2022.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mou2022_apsipa-abstract" role="button" aria-expanded="false" aria-controls="mou2022_apsipa-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="http://www.apsipa.org/proceedings/2022/APSIPA%202022/TuAM1-6/1570834752.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="mou2022_apsipa-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Speaker anonymization has been developed to protect personally identifiable information while retaining other encapsulated information in speech. Datasets, metrics, and protocols for evaluating speaker anonymization have been defined in the Voice Privacy Challenge (VPC). However, existing privacy metrics focus on evaluating general speaker individuality anonymization, which is represented by an x-vector. This study aims to investigate the effect of anonymization on the perception of gender. Understanding how anonymization caused gender transformation is essential for various applications of speaker anonymization. We proposed speaker anonymization methods across genders based on phase-vocoder time-scale modification (PV-TSM). Subsequently, in addition to the VPC evaluation, we developed a gender classifier to evaluate a speaker's gender anonymization. The objective evaluation results showed that our proposed method can successfully anonymize gender. In addition, our proposed methods outperformed the signal processing-based baseline methods in anonymizing speaker individuality represented by the x-vector in ASVeval while maintaining speech intelligibility.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <br>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://symposium2022.spsc-sig.org/#program" target="_blank">
                  SPSC-SIG
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="mou2022_spsc" class="col p-0">
                  <h5 class="title mb-0">Speaker Anonymization by Pitch Shifting Based on Time-Scale Modification.</h5>
                  <div class="author">
                    <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                    <nobr>Shogo Okada,</nobr>
                    and
                    <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      2nd Symposium on Security and Privacy in Speech Communication joined with 2nd VoicePrivacy Challenge Workshop September 23 & 24 2022, as a satellite to Interspeech 2022, Incheon, Korea.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mou2022_spsc-abstract" role="button" aria-expanded="false" aria-controls="mou2022_spsc-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.21437/SPSC.2022-7" target="_blank">DOI</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.isca-speech.org/archive/pdfs/spsc_2022/mawalim22_spsc.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="mou2022_spsc-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        The increasing usage of speech in digital technology raises a privacy issue because speech contains biometric information. Several methods of dealing with this issue have been proposed, including speaker anonymization or de-identification. Speaker anonymization aims to suppress personally identifiable information (PII) while keeping the other speech properties, including linguistic information. In this study, we utilize time-scale modification (TSM) speech signal processing for speaker anonymization. Speech signal processing approaches are significantly less complex than the state-of-the-art x-vector-based speaker anonymization method because it does not require a training process. We propose anonymization methods using two major categories of TSM, synchronous overlap-add (SOLA)-based algorithm and phase vocoder-based TSM (PV-TSM). For evaluating our proposed methods, we utilize the standard objective evaluation introduced in the VoicePrivacy challenge. The results show that our method based on the PV-TSM balances privacy and utility metrics better than baseline systems, especially when evaluating with an automatic speaker verification (ASV) system in anonymized enrollment and anonymized trials (a-a). Further, our method outperformed the x-vector-based speaker method, which has limitations in its complex training process, low privacy in an a-a scenario, and low voice distinctiveness.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <br>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="https://www.elsevier.com/" target="_blank">
                  Elsevier
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="mgkku2022_csl" class="col p-0">
                  <h5 class="title mb-0">Speaker Anonymization by Modifying Fundamental Frequency and X-Vectors Singular Value.</h5>
                  <div class="author">
                    <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Kasorn Galajit,</nobr> <nobr>Jessada Karnjana, </nobr> <nobr>Shunsuke Kidani,</nobr>
                    and
                    <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                        Computer Speech & Language, Elsevier, vol. 73, 101326, 2022.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mgkku2022_csl-abstract" role="button" aria-expanded="false" aria-controls="mgkku2022_csl-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.sciencedirect.com/science/article/pii/S0885230821001194" target="_blank">DOI</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.sciencedirect.com/science/article/pii/S0885230821001194/pdfft?md5=f2a2a2b65c955db3b44e37b04dd49a02&pid=1-s2.0-S0885230821001194-main.pdf" target="_blank">PDF</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/GZlZFXaIcso" target="_blank">Video</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="mgkku2022_csl-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Speaker anonymization is a method of protecting voice privacy by concealing individual speaker characteristics while preserving linguistic information. The VoicePrivacy Challenge 2020 was initiated to generalize the task of speaker anonymization. In the challenge, two frameworks for speaker anonymization were introduced; in this study, we propose a method of improving the primary framework by modifying the state-of-the-art speaker individuality feature (namely, x-vector) in a neural waveform speech synthesis model. Our proposed method is constructed based on x-vector singular value modification with a clustering model. We also propose a technique of modifying the fundamental frequency and speech duration to enhance the anonymization performance. To evaluate our method, we carried out objective and subjective tests. The overall objective test results show that our proposed method improves the anonymization performance in terms of the speaker verifiability, whereas the subjective evaluation results show improvement in terms of the speaker dissimilarity. The intelligibility and naturalness of the anonymized speech with speech prosody modification were slightly reduced (less than 5% of word error rate) compared to the results obtained by the baseline system.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <hr>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="https://www.mdpi.com/" target="_blank">
                  MDPI
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="mu2021_mdpi" class="col p-0">
                  <h5 class="title mb-0">Speech Watermarking by McAdams Coefficient Scheme Based on Random Forest Learning.</h5>
                  <div class="author">
                    <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                    and
                    <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      Entropy, MDPI, vol. 23, no. 10, 2021.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mu2021_mdpi-abstract" role="button" aria-expanded="false" aria-controls="mu2021_mdpi-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.3390/e23101246" target="_blank">DOI</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.mdpi.com/1099-4300/23/10/1246/pdf?version=1634304903" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="mu2021_mdpi-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Speech watermarking has become a promising solution for protecting the security of speech communication systems. We propose a speech watermarking method that uses the McAdams coefficient, which is commonly used for frequency harmonics adjustment. The embedding process was conducted, using bit-inverse shifting. We also developed a random forest classifier, using features related to frequency harmonics for blind detection. An objective evaluation was conducted to analyze the performance of our method in terms of the inaudibility and robustness requirements. The results indicate that our method satisfies the speech watermarking requirements with a 16 bps payload under normal conditions and numerous non-malicious signal processing operations, e.g., conversion to Ogg or MP4 format.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <hr>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://www.apsipa2021.org/" target="_blank">
                  APSIPA
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="mu2021_apsipa" class="col p-0">
                  <h5 class="title mb-0">Improving Security in McAdams Coefficient-Based Speaker Anonymization by Watermarking Method.</h5>
                  <div class="author">
                    <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                    and
                    <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Tokyo, Japan, December 2021.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mu2021_apsipa-abstract" role="button" aria-expanded="false" aria-controls="mu2021_apsipa-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/9689554" target="_blank">IEEE</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9689554&tag=1" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="mu2021_apsipa-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Speaker anonymization aims to suppress speaker individuality to protect privacy in speech while preserving the other aspects, such as speech content. One effective solution for anonymization is to modify the McAdams coefficient. In this work, we propose a method to improve the security for speaker anonymization based on the McAdams coefficient by using a speech watermarking approach. The proposed method consists of two main processes: one for embedding and one for detection. In embedding process, two different McAdams coefficients represent binary bits "0" and "1". The watermarked speech is then obtained by frame-by-frame bit inverse switching. Subsequently, the detection process is carried out by a power spectrum comparison. We conducted objective evaluations with reference to the VoicePrivacy 2020 Challenge (VP2020) and of the speech watermarking with reference to the Information Hiding Challenge (IHC) and found that our method could satisfy the blind detection, inaudibility, and robustness requirements in watermarking. It also significantly improved the anonymization performance in comparison to the secondary baseline system in VP2020.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

          <hr>

          <li>
            <div class="row m-0 mt-3 p-0">
              <div class="col-sm-1 p-0 abbr">
                <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="http://www.interspeech2020.org/" target="_blank">
                  Interspeech
                </a>
              </div>
              <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
                <div id="mgku2020_interspeech" class="col p-0">
                  <h5 class="title mb-0">X-Vector Singular Value Modification and Statistical-Based Decomposition with Ensemble Regression Modeling for Speaker Anonymization System.</h5>
                  <div class="author">
                    <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Kasorn Galajit,</nobr> <nobr>Jessada Karnjana, </nobr>
                    and
                    <nobr>Masashi Unoki.</nobr>
                  </div>
                  <div>
                    <p class="periodical font-italic">
                      Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, pp. 1703–1707, October 2020.
                    </p>
                  </div>
                  <div class="col p-0">
                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mgku2020_interspeech-abstract" role="button" aria-expanded="false" aria-controls="mgku2020_interspeech-abstract">Abstract</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.isca-speech.org/archive/interspeech_2020/mawalim20_interspeech.html" target="_blank">DOI</a>
                    <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/mawalim20_interspeech.pdf" target="_blank">PDF</a>
                  </div>
                  <div class="col mt-2 p-0">
                    <div id="mgku2020_interspeech-abstract" class="collapse">
                      <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                        Anonymizing speaker individuality is crucial for ensuring voice privacy protection. In this paper, we propose a speaker individuality anonymization system that uses singular value modification and statistical-based decomposition on an x-vector with ensemble regression modeling. An anonymization system requires speaker-to-speaker correspondence (each speaker corresponds to a pseudo-speaker), which may be possible by modifying significant x-vector elements. The significant elements were determined by singular value decomposition and variant analysis. Subsequently, the anonymization process was performed by an ensemble regression model trained using x-vector pools with clustering-based pseudo-targets. The results demonstrated that our proposed anonymization system effectively improves objective verifiability, especially in anonymized trials and anonymized enrollments setting, by preserving similar intelligibility scores with the baseline system introduced in the VoicePrivacy 2020 Challenge.
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </li>

        </ol>
      </div>
    </div>

    <p><br /></p>

  </div>

</div>

  <!-- Footer -->
  <footer>
    &copy; Copyright 2022 Candy Olivia Mawalim. Powered by <a href="http://jekyllrb.com/" target="_blank"><font color="#a20d0d">Jekyll</font></a> with <a href="https://github.com/alshedivat/al-folio"><font color="#a20d0d">al-folio</font></a> theme. Hosted by <a href="https://pages.github.com/" target="_blank"><font color="#a20d0d">GitHub Pages</font></a>.
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-54519238-1', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
