<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Candy Olivia Mawalim | Publications</title>
  <meta name="description" content="Personal website of Candy Olivia Mawalim.">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/teaching/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Candy Olivia</span> Mawalim</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
            </a>
          </li>
          
          <li class="nav-item ">
            <a class="nav-link" href="/profile/">
            Profile
            </a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="/teaching/">
            Teaching  
            </a>
          </li>
          
          <li class="nav-item ">
            <a class="nav-link" href="/research/">
            Research
            </a>
          </li>

          <li class="nav-item navbar-active font-weight-bold">
            <a class="nav-link" href="/publications/">
            Publications
            <span class="sr-only">(current)</span>
            </a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="/news/">
            News  
            </a>
          </li>
          
          <li class="nav-item ">
            <a class="nav-link" href="/contact/">
              Contact
            </a>
          </li>

        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
  <h1>Publications</h1>
  <h6><span>*</span> denotes equal contribution and joint lead authorship.</h6>
  <span class="badge font-weight-bold danger-color-dark align-middle" style="width: 90px;">Journal</span>
  <span class="badge font-weight-bold success-color-dark align-middle" style="width: 90px;">Book Chapter</span>
  <span class="badge font-weight-bold primary-color-dark align-middle" style="width: 90px;">Conference</span>

  <p><br/></p>

  <!-- 2024 -->
  <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2024</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="https://aes2.org/publications/journal/" target="_blank">
                JAES
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="fammwpkcp_jaes2024" class="col p-0">
                <h5 class="title mb-0">Forensic speech enhancement: </h5><h3>Introducing an innovative interdisciplinary project aiming to ensure reliable understanding of poor-quality speech recordings used as evidence in criminal trials</h3>
                <div class="author"><nobr>Helen Fraser,</nobr> <nobr>Vincent Aubanel,</nobr> <nobr>Robert C. Maher,</nobr> <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Xin Wang,</nobr> <nobr>Peter Počta,</nobr> <nobr>Emma Keith,</nobr> <nobr>Gérard Chollet,</nobr> 
                  and
                  <nobr>Karla Pizzi.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    Journal of the Audio Engineering Society, 2024 (Accepted).
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#fammwpkcp_jaes2024-abstract" role="button" aria-expanded="false" aria-controls="fammwpkcp_jaes2024-abstract">Abstract</a>
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1016/j.apacoust.2023.109663" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/1-s2.0-S0003682X23004619-main.pdf" target="_blank">PDF</a> -->
                </div>
                <div class="col mt-2 p-0">
                  <div id="fammwpkcp_jaes2024-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      TBD
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://icmi.acm.org/2024/" target="_blank">
                ICMI
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="llmhlo2024_icmi" class="col p-0">
                <h5 class="title mb-0">Do We Need to Watch It All? Efficient Job Interview Video Processing with Differentiable Masking.</h5>
                <div class="author">
                   <nobr>Hung Le,</nobr> <nobr>Sixia Li,</nobr> <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Hung-Hsuan Huang,</nobr> <nobr>Chee Wee Leong,</nobr> 
                  and <nobr>Shogo Okada.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    The 26th International Conference on Multimodal Interaction (ICMI 2024), San José, Costa Rica, 4-8 Nov 2024. (Accepted)
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#llmhlo2024_icmi-abstract" role="button" aria-expanded="false" aria-controls="llmhlo2024_icmi-abstract">Abstract</a>
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/NCSP_AMIU24.pdf" target="_blank">PDF</a> -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">DOI</a> -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">Code</a> -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">Demo</a> -->
                </div>
                <div class="col mt-2 p-0">
                  <div id="klmko2024_acii-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      TBD
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://acii-conf.net/2024/" target="_blank">
                ACII
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="klmko2024_acii" class="col p-0">
                <h5 class="title mb-0">Incremental Multimodal Sentiment Analysis on HAI Based on Multitask Active Learning with Inter-Annotator Agreement.</h5>
                <div class="author">
                   <nobr>Thus Karnjanapatchara,</nobr> <nobr>Sixia Li,</nobr> <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Kazunori Komatani,</nobr> 
                  and <nobr>Shogo Okada.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    The 12th International Conference on Affective Computing and Intelligent Interaction (ACII 2024), Glasglow, Scotland, UK, 15-18 September 2024. (Accepted)
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#klmko2024_acii-abstract" role="button" aria-expanded="false" aria-controls="klmko2024_acii-abstract">Abstract</a>
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/NCSP_AMIU24.pdf" target="_blank">PDF</a> -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">DOI</a> -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">Code</a> -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">Demo</a> -->
                </div>
                <div class="col mt-2 p-0">
                  <div id="klmko2024_acii-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      TBD
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="https://www.jstage.jst.go.jp/browse/jsp/-char/en" target="_blank">
                RISP
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="amiu_risp2024" class="col p-0">
                <h5 class="title mb-0">Study on Inaudible Speech Watermarking Method Based on Spread-Spectrum Using Linear Prediction Residue.</h5>
                <div class="author"><nobr>Aulia Adila,</nobr> <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Takuto Isoyama,</nobr> 
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    Journal of Signal Processing, Research Institute of Signal Processing, 2024 (Accepted).
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#amiu_risp2024-abstract" role="button" aria-expanded="false" aria-controls="amiu_risp2024-abstract">Abstract</a>
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1016/j.apacoust.2023.109663" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/1-s2.0-S0003682X23004619-main.pdf" target="_blank">PDF</a> -->
                </div>
                <div class="col mt-2 p-0">
                  <div id="amiu_risp2024-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      A reliable speech watermarking technique must balance satisfying four requirements: inaudibility, robustness, blind detectability, and confidentiality. A previous study proposed a speech watermarking technique based on direct spread spectrum (DSS) using a linear prediction (LP) scheme, i.e., LP-DSS, that could simultaneously satisfy these four requirements. However, an inaudibility issue was found due to the incorporation of a blind detection scheme with frame synchronization. In this paper, we investigate the feasibility of utilizing a psychoacoustical model, which simulates auditory masking, to control the suitable embedding level of the watermark signal to resolve the inaudibility issue in the LP-DSS scheme. Evaluation results confirmed that controlling the embedding level with the psychoacoustical model, with a constant scaling factor setting, could balance the trade-off between inaudibility and detection ability with a payload up to 64 bps.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://interspeech2024.org/" target="_blank">
                Interspeech
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mou2024_interspeech" class="col p-0">
                <h5 class="title mb-0">Are Recent Deep Learning-Based Speech Enhancement Methods Ready to Confront Real-World Noisy Environments?</h5>
                <div class="author">
                   <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Shogo Okada,</nobr> 
                  and <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    The 25th Interspeech Conference, Kos Island, Greece, 1-5 September 2024. (Accepted)
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mou2024_interspeech-abstract" role="button" aria-expanded="false" aria-controls="mou2024_interspeech-abstract">Abstract</a>
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/NCSP_AMIU24.pdf" target="_blank">PDF</a> -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">DOI</a> -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">Code</a> -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">Demo</a> -->
                </div>
                <div class="col mt-2 p-0">
                  <div id="mou2024_interspeech-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Recent advancements in speech enhancement techniques have ignited interest in improving speech quality and intelligibility. However, the effectiveness of recently proposed methods is unclear. In this paper, a comprehensive analysis of modern deep learning-based speech enhancement approaches is presented. Through evaluations using the Deep Suppression Noise and Clarity Enhancement Challenge datasets, we assess the performances of three methods: Denoiser, DeepFilterNet3, and FullSubNet+. Our findings reveal nuanced performance differences among these methods, with varying efficacy across datasets. While objective metrics offer valuable insights, they struggle to represent complex scenarios with multiple noise sources. Leveraging ASR-based methods for these scenarios shows promise but may induce critical hallucination effects. Our study emphasizes the need for ongoing research to refine techniques for diverse real-world environments.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://www.ieai.net/" target="_blank">
                IEAI
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="tgmy2024_ieai" class="col p-0">
                <h5 class="title mb-0">Exploring a Cutting-Edge Convolutional Neural Network for Speech Emotion Recognition</h5>
                <div class="author">
                   <nobr>Navod Neranjan Thilakarathne,</nobr> <nobr>Kasorn Galajit,</nobr> <nobr><em>Candy Olivia Mawalim</em>,</nobr> 
                  and <nobr>Hayati Yassin.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    The 5th International conference on Industrial Engineering and Artificial Intelligence (IEAI 2024), Chulalongkorn University, Bangkok, Thailand, 24-26 April 2024.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#tgmy2024_ieai-abstract" role="button" aria-expanded="false" aria-controls="tgmy2024_ieai-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/IEAI2024_MS5023.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="tgmy2024_ieai-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      In light of the ongoing expansion of humancomputer interaction, advancements in the comprehension and interpretation of human emotions are of the utmost importance. SER, representing speech emotion recognition, is a critical element in this context as it enables computational systems to comprehend the emotions of humans. Throughout the years, SER has employed a variety of techniques, including well-established speech analysis and classification methods. However, in recent years, techniques powered by deep learning have been suggested as a viable substitute for conventional SER methods, owing to their more encouraging outcomes in comparison to the aforementioned methods. In this regard, by utilizing a novel Convolutional Neural Network (CNN) model designed to assess and categorize seven emotional states based on speech signals retrieved from three distinct databases, this research presents a novel approach to SER that yields 88.76% accuracy. The purpose of this research is to enhance the accuracy and efficiency of emotion identification, with the end goal of boosting applications in fields such as interactive voice response systems, mental health monitoring, and personalized digital assistants.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="http://www.risp.jp/NCSP24/" target="_blank">
                NCSP
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="amiu2024_ncsp" class="col p-0">
                <h5 class="title mb-0">Study on Inaudible Speech Watermarking Method Based on Spread-Spectrum Using Linear Prediction Residue.</h5>
                <div class="author">
                  <nobr>Aulia Adila,</nobr> <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Isoyama Takuto,</nobr> 
                  and <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    The 2024 RISP International Workshop on Nonlinear Circuits, Communications and Signal Processing, Hawaii, 27 February - 1 March 2024.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#amiu2024_ncsp-abstract" role="button" aria-expanded="false" aria-controls="amiu2024_ncsp-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/NCSP_AMIU24.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="amiu2024_ncsp-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      A reliable speech watermarking technique must balance satisfying the four requirements: inaudibility, robustness, blind-detectability, and confidentiality. The previous study proposed an LP-DSS scheme that could simultaneously satisfy these four requirements. However, the inaudibility issue happened due to the blind detection scheme with frame synchronization. In this paper, we investigate the feasibility of utilizing a psychoacoustical model to control the suitable embedding level of the watermark signal to resolve the inaudibility issue that arises in the LP-DSS scheme. A psychoacoustical model simulates the auditory masking phenomenon that “mask” signals below the masking curve to be imperceptible to human ears. Results of the evaluation confirmed that the controlled embedding level from the psychoacoustical model balanced the trade-off between inaudibility and detection ability with payload up to 64 bps.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

      </ol>
    </div>
  </div>

  <!-- 2023 -->
  <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2023</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://isai-nlp-aiot2023.aiat.or.th/" target="_blank">
                iSAI-NLP
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="ivo_isainlp2023" class="col p-0">
                <h5 class="title mb-0">ThaiSpoof: A Database for Spoof Detection in Thai Language.</h5>
                <div class="author">
                  <nobr>Kasorn Galajit,</nobr>
                  <nobr>Thunpisit Kosolsriwiwat,</nobr>
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                  <nobr>Pakinee Aimmanee,</nobr>
                  <nobr>Waree Kongprawechnon,</nobr>
                  <nobr>Win Pa Pa,</nobr>
                  <nobr>Anuwat Chaiwongyen,</nobr>
                  <nobr>Teeradaj Racharak,</nobr>
                  <nobr>Hayati Yassin,</nobr>
                  <nobr>Jessada Karnjana,</nobr>
                  <nobr>Surasak Boonkla,</nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    The 18th International Joint Symposium on Artificial Intelligence and Natural Language Processing and The International Conference on Artificial Intelligence and Internet of Things (iSAI-NLP 2023).
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#ivo_isainlp2023-abstract" role="button" aria-expanded="false" aria-controls="ivo_isainlp2023-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354956" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/10354956" target="_blank">IEEE</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/2023287845_thaispoof.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="ivo_isainlp2023-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Many applications and security systems have widely applied automatic speaker verification (ASV). However, these systems are vulnerable to various direct and indirect access attacks, which weakens their authentication capability. The research in spoofed speech detection contributes to enhancing these systems. However, the study in spoofing detection is limited to only some languages due to the need for various datasets. This paper focuses on a Thai language dataset for spoof detection. The dataset consists of genuine speech signals and various types of spoofed speech signals. The spoofed speech dataset is generated using text-to-speech tools for the Thai language, synthesis tools, and tools for speech modification. To showcase the utilization of this dataset, we implement a simple model based on a convolutional neural network (CNN) taking linear frequency cepstral coefficients (LFCC) as its input. We trained, validated, and tested the model on our dataset referred to as ThaiSpoof. The experimental result shows that the accuracy of model is 95%, and equal error rate (EER) is 4.67%. The result shows that our ThaiSpoof dataset has the potential to develop for helping in spoof detection studies.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://isai-nlp-aiot2023.aiat.or.th/" target="_blank">
                iSAI-NLP
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mgmkia_isainlp2023" class="col p-0">
                <h5 class="title mb-0">Voice Contribution on LFCC feature and ResNet-34 for Spoof Detection.</h5>
                <div class="author">
                  <nobr>Khaing Zar Mon,</nobr>
                  <nobr>Kasorn Galajit,</nobr>
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                  <nobr>Jessada Karnjana,</nobr>
                  <nobr>Tsuyoshi Isshiki,</nobr>
                  and
                  <nobr>Pakinee Aimmanee.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    The 18th International Joint Symposium on Artificial Intelligence and Natural Language Processing and The International Conference on Artificial Intelligence and Internet of Things (iSAI-NLP 2023).
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mgmkia_isainlp2023-abstract" role="button" aria-expanded="false" aria-controls="mgmkia_isainlp2023-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/iSAI-NLP60301.2023.10354625" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10354625" target="_blank">IEEE</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/Spoof_Detection_using_Voice_Contribution_on_LFCC_features_and_ResNet-34.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mgmkia_isainlp2023-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Recently, biometric authentication has been significant advancement, particularly in speaker verification. While there have been significant advancements in this technology, compelling evidence highlights the continued vulnerability of this technology to malicious spoofing attacks. This vulnerability calls for developing specialized countermeasures to identify various attack types. This paper specifically focuses on detecting replay, speech synthesis, and voice conversion attacks. As our spoof detection strategy’s front-end feature extraction method, we used linear frequency cepstral coefficients (LFCC). We utilized ResNet-34 to classify between genuine and fake speech. By integrating LFCC with ResNet-34, We evaluated the proposed method using the ASVspoof 2019 dataset, PA (Physical Access), and LA (Logical Access). In our approach, we compare the use of the entire utterance for feature extraction in both PA and LA datasets with an alternative method that involves extracting features from a specific percentage of the voice segment within the utterance for classification. In addition, we conducted a comprehensive evaluation by comparing our proposed method with the established baseline techniques, LFCC-GMM and CQCC-GMM. The proposed method demonstrates promising performance, achieving equal error rate (EER) of 3.11% and 3.49% for the development and evaluation datasets, respectively, in PA attacks. In LA attacks evaluation, the proposed method performs EER of 0.16% and 6.89% for the development and evaluation datasets, respectively. The proposed method shows promising results in identifying spoof attacks for both PA and LA attacks.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://www.apsipa2023.org/" target="_blank">
                APSIPA
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="cmlwu2023_apsipa" class="col p-0">
                <h5 class="title mb-0">Analysis of Spectro-Temporal Modulation Representation for Deep-Fake Speech Detection.</h5>
                <div class="author">
                  <nobr>Haowei Cheng,</nobr>
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                  <nobr>Kai Li,</nobr>
                  <nobr>Lijun Wang,</nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    The 15th Asia-Pasific Signal and Information Processing Association (APSIPA ASC 2023), Taipei, Taiwan, 31 October - 3 November 2023.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#cmlwu2023_apsipa-abstract" role="button" aria-expanded="false" aria-controls="cmlwu2023_apsipa-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/APSIPAASC58517.2023.10317309" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/10317309" target="_blank">IEEE</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/APSIPA2023_Cheng.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="cmlwu2023_apsipa-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Deep-fake speech detection aims to develop effective techniques for identifying fake speech generated using advanced deep-learning methods. It can reduce the negative impact of malicious production or dissemination of fake speech in real-life scenarios. Although humans can relatively easy to distinguish between genuine and fake speech due to human auditory mechanisms, it is difficult for machines to distinguish them correctly. One major reason for this challenge is that machines struggle to effectively separate speech content from human vocal system information. Common features used in speech processing face difficulties in handling this issue, hindering the neural network from learning the discriminative differences between genuine and fake speech. To address this issue, we investigated spectro-temporal modulation representations in genuine and fake speech, which simulate the human auditory perception process. Next, the spectro-temporal modulation was fit to a light convolutional neural network bidirectional long short-term memory for classification. We conducted experiments on the benchmark datasets of the Automatic Speaker Verification and Spoofing Countermeasures Challenge 2019 (ASVspoof2019) and the Audio Deep synthesis Detection Challenge 2023 (ADD2023), achieving an equal-error rate of 8.33\% and 42.10\%, respectively. The results showed that spectro-temporal modulation representations could distinguish the genuine and deep-fake speech and have adequate performance in both datasets.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://www.apsipa2023.org/" target="_blank">
                APSIPA
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="zmu2023_apsipa" class="col p-0">
                <h5 class="title mb-0">Incorporating the Digit Triplet Test in A Lightweight Speech Intelligibility Prediction for Hearing Aids.</h5>
                <div class="author">
                  <nobr>Xiajie Zhou,</nobr>
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    The 15th Asia-Pasific Signal and Information Processing Association (APSIPA ASC 2023), Taipei, Taiwan, 31 October - 3 November 2023.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#zmu2023_apsipa-abstract" role="button" aria-expanded="false" aria-controls="zmu2023_apsipa-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/APSIPAASC58517.2023.10317260" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/10317260" target="_blank">IEEE</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/APSIPA2023_Zhou.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="zmu2023_apsipa-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Recent studies in speech processing often utilize sophisticated methods for solving a task to obtain high-accuracy results. Although high performance could be achieved, the methods are too complex and require high-performance computational power that might not be available for a wide range of researchers. In this study, we propose a method to incorporate the low dimensional and the recent state-of-the-art acoustic features for speech processing to predict the speech intelligibility in noise for hearing aids. The proposed method was developed based on the stack regressor on various traditional machine learning regressors. Unlike other existing works, we utilized the results of the digit triplet test, which is usually used to measure the hearing ability in the existence of noise, to improve the prediction. The evaluation of our proposed method was carried out by using the first Clarity Prediction Challenge dataset. This dataset is utilized for speech intelligibility prediction that consists of speech signals output of hearing aids that were arranged in various simulated scenes with interferers. Our experimental results show that the proposed method could improve speech intelligibility prediction. The results also show that the digit triplet test results are beneficial for speech intelligibility prediction in noise.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="https://www.sciencedirect.com/journal/applied-acoustics" target="_blank">
                APAC
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mtou_apac2023" class="col p-0">
                <h5 class="title mb-0">Non-Intrusive Speech Intelligibility Prediction Using an Auditory Periphery Model with Hearing Loss.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Benita Angela Titalim,</nobr> <nobr>Shogo Okada,</nobr> 
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    Applied Acoustics, 2023.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mtou_apac2023-abstract" role="button" aria-expanded="false" aria-controls="mtou_apac2023-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1016/j.apacoust.2023.109663" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/1-s2.0-S0003682X23004619-main.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mtou_apac2023-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Speech intelligibility prediction methods are necessary for hearing aid development. However, many such prediction methods are categorized as intrusive metrics because they require reference speech as input, which is often unavailable in real-world situations. Additionally, the processing techniques in hearing aids may cause temporal or frequency shifts, which degrade the accuracy of intrusive speech intelligibility metrics. This paper proposes a non-intrusive auditory model for predicting speech intelligibility under hearing loss conditions. The proposed method requires binaural signals from hearing aids and audiograms representing the hearing conditions of hearing-impaired listeners. It also includes additional acoustic features to improve the method’s robustness in noisy and reverberant environments. A two-dimensional convolutional neural network with neural decision forests is used to construct a speech intelligibility prediction model. An evaluation conducted with the first Clarity Prediction Challenge dataset shows that the proposed method performs better than the baseline system.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="https://ieeeaccess.ieee.org/" target="_blank">
                IEEE Access
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="hmimno2023_access" class="col p-0">
                <h5 class="title mb-0">A Ranking Model for Evaluation of Conversation Partners Based on Rapport Levels.</h5>
                <div class="author">
                  <nobr>Takato Hayashi,</nobr> 
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Ryo Ishii,</nobr> <nobr>Akira Morikawa,</nobr> <nobr>Takao Nakamura,</nobr> 
                  and
                  <nobr>Shogo Okada.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    IEEE Access, 2023.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#hmimno2023_access-abstract" role="button" aria-expanded="false" aria-controls="hmimno2023_access-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1109/ACCESS.2023.3287984" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/Access2023_Hayashi.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="hmimno2023_access-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Our proposed ranking model ranks conversation partners based on self-reported rapport levels for each participant. The model is important for tasks that recommend interaction partners based on user rapport built in past interactions, such as matchmaking between a student and a teacher in one-to-one online language classes. To rank conversation partners, we can use a regression model that predicts rapport ratings. It is, however, challenging to learn the mapping from the participants' behavior to their associated rapport ratings because a subjective scale for rapport ratings may vary across different participants. Hence, we propose a ranking model trained via preference learning (PL). The model avoids the subjective scale bias because the model is trained to predict ordinal relations between two conversation partners based on rapport ratings reported by the same participant. The input of the model is multimodal (acoustic and linguistic) features extracted from two participants' behaviors in an interaction. Since there is no publicly available dataset for validating the ranking model, we created a new dataset composed of online dyadic (person-to-person) interactions between a participant and several different conversation partners. We compare the ranking model trained via preference learning with the regression model by using evaluation metrics for the ranking. The experimental results show that preference learning is a more suitable approach for ranking conversation partners. Furthermore, we investigate the effect of each modality and the different stages of rapport development on the ranking performance.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://eusipco2023.org/" target="_blank">
                EUSIPCO
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mtou2023_eusipco" class="col p-0">
                <h5 class="title mb-0">Auditory Model Optimization with Wavegram-CNN and Acoustic Parameter Models for Nonintrusive Speech Intelligibility Prediction in Hearing Aids.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Benita Angela Titalim,</nobr> <nobr>Shogo Okada,</nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    The 31st European Signal Processing Conference (EUSIPCO 2023), Helsinki, Finland.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mtou2023_eusipco-abstract" role="button" aria-expanded="false" aria-controls="mtou2023_eusipco-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.23919/EUSIPCO58844.2023.10289742" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/EUSIPCO2023_Mawalim.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mtou2023_eusipco-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Nonintrusive speech intelligibility (SI) prediction is essential for evaluating many speech technology applications, including hearing aid development. In this study, several factors related to hearing perception are investigated to predict SI. In the proposed method, we integrated a physiological auditory model from two ears (binaural EarModel), wavegram-CNN model and acoustic parameter model. The refined EarModel does not require clean speech as input (blind method). In EarModel, the perception caused by hearing loss is simulated based on audiograms. Meanwhile, the wavegram-CNN and acoustic parameter models represent the factors related to the speech spectrum and acoustics, respectively. The proposed method is evaluated based on the scenario from the 1st Clarity Prediction Challenge (CPC1). The results show that the proposed method outperforms the intrusive baseline MBSTOI and HASPI methods in terms of the Pearson coefficient (ρ), RMSE, and R2 score in both closed-set and open-set tracks. Based on the results from listener-wise evaluation results, the average $\rho$ could be improved by more than 0.3 using the proposed method.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold success-color-dark align-middle" style="width: 75px;" href="https://www.springer.com/gp/computer-science/lncs" target="_blank">
                LNCS
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="lmo2023_hcii" class="col p-0">
                <h5 class="title mb-0">Inter-person Intra-modality Attention Based Model for Dyadic Interaction Engagement Prediction.</h5>
                <div class="author">
                  <nobr>Xiguang Li,</nobr> <nobr>Shogo Okada,</nobr>
                  and
                  <nobr><em>Candy Olivia Mawalim</em>.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    The 25th HCI International Conference, HCII 2023, Copenhagen, Denmark, July 23–28, 2023.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#lmo2023_hcii-abstract" role="button" aria-expanded="false" aria-controls="lmo2023_hcii-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1007/978-3-031-35915-6_8" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/LNCS2023_HCII_Li.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="lmo2023_hcii-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      With the rapid development of artificial agents, more researchers have explored the importance of user engagement level prediction. Real-time user engagement level prediction assists the agent in properly adjusting its policy for the interaction. However, the existing engagement modeling lacks the element of interpersonal synchrony, a temporal behavior alignment closely related to the engagement level. Part of this is because the synchrony phenomenon is complex and hard to delimit. With this background, we aim to develop a model suitable for temporal interpersonal features with the help of the modern data-driven machine learning method. Based on previous studies, we select multiple non-verbal modalities of dyadic interactions as predictive features and design a multi-stream attention model to capture the interpersonal temporal relationship of each modality. Furthermore, we experiment with two additional embedding schemas according to the synchrony definitions in psychology. Finally, we compare our model with a conventional structure that emphasizes the multimodal features within an individual. Our experiments showed the effectiveness of the intra-modal inter-person design in engagement prediction. However, the attempt to manipulate the embeddings failed to improve the performance. In the end, we discuss the experiment result and elaborate on the limitations of our work.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold success-color-dark align-middle" style="width: 75px;" href="https://www.springer.com/gp/computer-science/lncs" target="_blank">
                LNCS
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="llmhlo2023_hcii" class="col p-0">
                <h5 class="title mb-0">Investigating the Effect of Linguistic Features on Personality and Job Performance Predictions.</h5>
                <div class="author">
                  <nobr>Hung Le,</nobr> <nobr>Sixia Li,</nobr> <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Hung-Hsuan Huang,</nobr> <nobr>Chee Wee Leong,</nobr>
                  and
                  <nobr>Shogo Okada.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    The 25th HCI International Conference, HCII 2023, Copenhagen, Denmark, July 23–28, 2023.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#llmhlo2023_hcii-abstract" role="button" aria-expanded="false" aria-controls="llmhlo2023_hcii-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1007/978-3-031-35915-6_27" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/LNCS2023_HCII_Le.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="llmhlo2023_hcii-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Personality traits are known to have a high correlation with job performance. On the other hand, there is a strong relationship between language and personality. In this paper, we presented a neural network model for inferring personality and hirability. Our model was trained only from linguistic features but achieved good results by incorporating transfer learning and multi-task learning techniques. The model improved the F1 score 5.6% point on the Hiring Recommendation label compared to previous work. The effect of different Automatic Speech Recognition systems on the performance of the models was also shown and discussed. Lastly, our analysis suggested that the model makes better judgments about hirability scores when the personality traits information is not absent.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="https://www.springer.com/journal/12193" target="_blank">
                JMUI
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="monu2023_jmui" class="col p-0">
                <h5 class="title mb-0">Personality Trait Estimation in Group Discussions using Multimodal Analysis and Speaker Embedding.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Shogo Okada,</nobr> <nobr>Yukiko I. Nakano,</nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    Journal on Multimodal User Interfaces, 2023.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#monu2023_jmui-abstract" role="button" aria-expanded="false" aria-controls="monu2023_jmui-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://link.springer.com/article/10.1007/s12193-023-00401-0" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/paper/JMUI2023_Mawalim.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="monu2023_jmui-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      The automatic estimation of personality traits is essential for many human-computer interface (HCI) applications. This paper focused on improving Big Five personality trait estimation in group discussions via multimodal analysis and transfer learning with the state-of-the-art speaker individuality feature, namely, the identity vector (i-vector) speaker embedding. The experiments were carried out by investigating the effective and robust multimodal features for estimation with two group discussion datasets, i.e., the Multimodal Task-Oriented Group Discussion (MATRICS) (in Japanese) and Emergent Leadership (ELEA) (in European languages) corpora. Subsequently, the evaluation was conducted by using leave-one-person-out cross-validation (LOPCV) and ablation tests to compare the effectiveness of each modality. The overall results showed that the speaker-dependent features, e.g., the i-vector, effectively improved the prediction accuracy of Big Five personality trait estimation. In addition, the experimental results showed that audio-related features were the most prominent features in both corpora.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

      </ol>
    </div>
  </div>
  <!--  -->

  <!-- 2022 -->
  <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://mum-conf.org/2022/" target="_blank">
                ACM-MUM
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="omkko2022_acmmum" class="col p-0">
                <h5 class="title mb-0">Multimodal Analysis for Communication Skill and Self-Efficacy Level Estimation in Job Interview Scenario.</h5>
                <div class="author">
                  <nobr>Tomoya Ohba*,</nobr> <nobr><em>Candy Olivia Mawalim*</em>,</nobr> <nobr>Shun Katada,</nobr> <nobr>Haruki Kuroki,</nobr>
                  and
                  <nobr>Shogo Okada.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    The 21st International Conference on Mobile and Ubiquitous Multimedia (ACM MUM 2022), Lisbon, Portugal, 27--30 November 2022.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#omkko2022_acmmum-abstract" role="button" aria-expanded="false" aria-controls="omkko2022_acmmum-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1145/3568444.3568461" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://dl.acm.org/doi/pdf/10.1145/3568444.3568461" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="omkko2022_acmmum-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      An interview for a job recruiting process requires applicants to demonstrate their communication skills. Interviewees sometimes become nervous about the interview because interviewees themselves do not know their assessed score. This study investigates the relationship between the communication skill (CS) and the  self-efficacy level (SE) of interviewees through multimodal modeling. We also clarify the difference between effective features in the prediction of CS and SE labels. For this purpose, we collect a novel multimodal job interview data corpus by using a job interview agent system where users experience the interview using a virtual reality head-mounted display (VR-HMD). The data corpus includes annotations of CS by third-party experts and SE annotations by the interviewees. The data corpus also includes various kinds of multimodal data, including audio, biological (i.e., physiological), gaze, and language data. We present two types of regression models, linear regression and sequential-based regression models, to predict CS, SE, and the gap (GA) between skill and self-efficacy. Finally, we report that the model with acoustic, gaze, and linguistic features has the best regression accuracy in CS prediction (correlation coefficient r = 0.637). Furthermore, the regression model with biological features achieves the best accuracy in SE prediction (r = 0.330).
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://sst2022.com/" target="_blank">
                SST
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mtuo2022_sst" class="col p-0">
                <h5 class="title mb-0">OBISHI: Objective Binaural Intelligibility Score for the Hearing Impaired.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Benita Angela Titalim,</nobr> <nobr>Masashi Unoki, </nobr>
                  and
                  <nobr>Shogo Okada.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    SST2022, The 18th Australasian International Conference on Speech Science and Technology, Canberra, Australia, 13--16 December 2022.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mtuo2022_sst-abstract" role="button" aria-expanded="false" aria-controls="mtuo2022_sst-abstract">Abstract</a>
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.isca-speech.org/archive/interspeech_2020/mawalim20_interspeech.html" target="_blank">DOI</a> -->
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://sst2022.files.wordpress.com/2022/12/mawalim-et-al-2022-obishi-objective-binaural-intelligibility-score-for-the-hearing-impaired.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mtuo2022_sst-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Speech intelligibility prediction for both normal hearing and hearing impairment is very important for hearing aid development. The Clarity Prediction Challenge 2022 (CPC1) was initiated to evaluate the speech intelligibility of speech signals produced by hearing aid systems. Modified binaural short-time objective intelligibility (MBSTOI) and hearing aid speech prediction index (HASPI) were introduced in the CPC1 to understand the basis of speech intelligibility prediction. This paper proposes a method to predict speech intelligibility scores, namely OBISHI. OBISHI is an intrusive (non-blind) objective measurement that receives binaural speech input and considers the hearing-impaired characteristics. In addition, a pre-trained automatic speech recognition (ASR) system was also utilized to infer the difficulty of utterances regardless of the hearing loss condition. We also integrated the hearing loss model by the Cambridge auditory group and the Gammatone Filterbank-based prediction model. The total evaluation was conducted by comparing the predicted intelligibility score of the baseline MBSTOI and HASPI with the actual correctness of listening tests. In general, the results showed that the proposed method, OBISHI, outperformed the baseline MBSTOI and HASPI (improved approximately 10% classification accuracy in terms of F1 score).
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://www.apsipa2022.org/" target="_blank">
                APSIPA
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="tmou2022_apsipa" class="col p-0">
                <h5 class="title mb-0">Speech Intelligibility Prediction for Hearing Aids Using an Auditory Model and Acoustic Parameters.</h5>
                <div class="author">
                  <nobr>Benita Angela Titalim*,</nobr>
                  <nobr><em>Candy Olivia Mawalim*</em>,</nobr>
                  <nobr>Shogo Okada,</nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Chiang Mai, Thailand, 7--10 November 2022.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#tmou2022_apsipa-abstract" role="button" aria-expanded="false" aria-controls="tmou2022_apsipa-abstract">Abstract</a>
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/9689554" target="_blank">IEEE</a> -->
                  <a class="badge grey waves-effect font-weight-light mr-1" href="http://www.apsipa.org/proceedings/2022/APSIPA%202022/WedPM2-2/1570834765.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="tmou2022_apsipa-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Objective speech intelligibility (SI) metrics for hearing-impaired people play an important role in hearing aid development. The work on improving SI prediction also became the basis of the first Clarity Prediction Challenge (CPC1). This study investigates a physiological auditory model called EarModel and acoustic parameters for SI prediction. EarModel is utilized because it provides advantages in estimating human hearing, both normal and impaired. The hearing-impaired condition is simulated in EarModel based on audiograms; thus, the SI perceived by hearing-impaired people is more accurately predicted. Moreover, the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) and WavLM, as additional acoustic parameters for estimating the difficulty levels of given utterances, are included to achieve improved prediction accuracy. The proposed method is evaluated on the CPC1 database. The results show that the proposed method improves the SI prediction effects of the baseline and hearing aid speech prediction index (HASPI). Additionally, an ablation test shows that incorporating the eGeMAPS and WavLM can significantly contribute to the prediction model by increasing the Pearson correlation coefficient by more than 15% and decreasing the root-mean-square error (RMSE) by more than 10.00 in both closed-set and open-set tracks.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://www.apsipa2022.org/" target="_blank">
                APSIPA
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mou2022_apsipa" class="col p-0">
                <h5 class="title mb-0">F0 Modification via PV-TSM Algorithm for Speaker Anonymization Across Gender.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                  <nobr>Shogo Okada,</nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Chiang Mai, Thailand, 7-10 November 2022.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mou2022_apsipa-abstract" role="button" aria-expanded="false" aria-controls="mou2022_apsipa-abstract">Abstract</a>
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/9689554" target="_blank">IEEE</a> -->
                  <a class="badge grey waves-effect font-weight-light mr-1" href="http://www.apsipa.org/proceedings/2022/APSIPA%202022/TuAM1-6/1570834752.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mou2022_apsipa-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Speaker anonymization has been developed to protect personally identifiable information while retaining other encapsulated information in speech. Datasets, metrics, and protocols for evaluating speaker anonymization have been defined in the Voice Privacy Challenge (VPC). However, existing privacy metrics focus on evaluating general speaker individuality anonymization, which is represented by an x-vector. This study aims to investigate the effect of anonymization on the perception of gender. Understanding how anonymization caused gender transformation is essential for various applications of speaker anonymization. We proposed speaker anonymization methods across genders based on phase-vocoder time-scale modification (PV-TSM). Subsequently, in addition to the VPC evaluation, we developed a gender classifier to evaluate a speaker's gender anonymization. The objective evaluation results showed that our proposed method can successfully anonymize gender. In addition, our proposed methods outperformed the signal processing-based baseline methods in anonymizing speaker individuality represented by the x-vector in ASVeval while maintaining speech intelligibility.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://symposium2022.spsc-sig.org/#program" target="_blank">
                SPSC-SIG
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mou2022_spsc" class="col p-0">
                <h5 class="title mb-0">Speaker Anonymization by Pitch Shifting Based on Time-Scale Modification.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                  <nobr>Shogo Okada,</nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    2nd Symposium on Security and Privacy in Speech Communication joined with 2nd VoicePrivacy Challenge Workshop September 23 & 24 2022, as a satellite to Interspeech 2022, Incheon, Korea.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mou2022_spsc-abstract" role="button" aria-expanded="false" aria-controls="mou2022_spsc-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.21437/SPSC.2022-7" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.isca-speech.org/archive/pdfs/spsc_2022/mawalim22_spsc.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mou2022_spsc-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      The increasing usage of speech in digital technology raises a privacy issue because speech contains biometric information. Several methods of dealing with this issue have been proposed, including speaker anonymization or de-identification. Speaker anonymization aims to suppress personally identifiable information (PII) while keeping the other speech properties, including linguistic information. In this study, we utilize time-scale modification (TSM) speech signal processing for speaker anonymization. Speech signal processing approaches are significantly less complex than the state-of-the-art x-vector-based speaker anonymization method because it does not require a training process. We propose anonymization methods using two major categories of TSM, synchronous overlap-add (SOLA)-based algorithm and phase vocoder-based TSM (PV-TSM). For evaluating our proposed methods, we utilize the standard objective evaluation introduced in the VoicePrivacy challenge. The results show that our method based on the PV-TSM balances privacy and utility metrics better than baseline systems, especially when evaluating with an automatic speaker verification (ASV) system in anonymized enrollment and anonymized trials (a-a). Further, our method outperformed the x-vector-based speaker method, which has limitations in its complex training process, low privacy in an a-a scenario, and low voice distinctiveness.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="https://www.elsevier.com/" target="_blank">
                Elsevier
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mgkku2022_csl" class="col p-0">
                <h5 class="title mb-0">Speaker Anonymization by Modifying Fundamental Frequency and X-Vectors Singular Value.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Kasorn Galajit,</nobr> <nobr>Jessada Karnjana, </nobr> <nobr>Shunsuke Kidani,</nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                      Computer Speech & Language, Elsevier, vol. 73, 101326, 2022.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mgkku2022_csl-abstract" role="button" aria-expanded="false" aria-controls="mgkku2022_csl-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.sciencedirect.com/science/article/pii/S0885230821001194" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.sciencedirect.com/science/article/pii/S0885230821001194/pdfft?md5=f2a2a2b65c955db3b44e37b04dd49a02&pid=1-s2.0-S0885230821001194-main.pdf" target="_blank">PDF</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/GZlZFXaIcso" target="_blank">Video</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mgkku2022_csl-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Speaker anonymization is a method of protecting voice privacy by concealing individual speaker characteristics while preserving linguistic information. The VoicePrivacy Challenge 2020 was initiated to generalize the task of speaker anonymization. In the challenge, two frameworks for speaker anonymization were introduced; in this study, we propose a method of improving the primary framework by modifying the state-of-the-art speaker individuality feature (namely, x-vector) in a neural waveform speech synthesis model. Our proposed method is constructed based on x-vector singular value modification with a clustering model. We also propose a technique of modifying the fundamental frequency and speech duration to enhance the anonymization performance. To evaluate our method, we carried out objective and subjective tests. The overall objective test results show that our proposed method improves the anonymization performance in terms of the speaker verifiability, whereas the subjective evaluation results show improvement in terms of the speaker dissimilarity. The intelligibility and naturalness of the anonymized speech with speech prosody modification were slightly reduced (less than 5% of word error rate) compared to the results obtained by the baseline system.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>
      </ol>
    </div>
  </div>
  <!--  -->

  <!-- 2021 -->
  <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2021</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="https://www.mdpi.com/" target="_blank">
                MDPI
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mu2021_mdpi" class="col p-0">
                <h5 class="title mb-0">Speech Watermarking by McAdams Coefficient Scheme Based on Random Forest Learning.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    Entropy, MDPI, vol. 23, no. 10, 2021.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mu2021_mdpi-abstract" role="button" aria-expanded="false" aria-controls="mu2021_mdpi-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.3390/e23101246" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.mdpi.com/1099-4300/23/10/1246/pdf?version=1634304903" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mu2021_mdpi-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Speech watermarking has become a promising solution for protecting the security of speech communication systems. We propose a speech watermarking method that uses the McAdams coefficient, which is commonly used for frequency harmonics adjustment. The embedding process was conducted, using bit-inverse shifting. We also developed a random forest classifier, using features related to frequency harmonics for blind detection. An objective evaluation was conducted to analyze the performance of our method in terms of the inaudibility and robustness requirements. The results indicate that our method satisfies the speech watermarking requirements with a 16 bps payload under normal conditions and numerous non-malicious signal processing operations, e.g., conversion to Ogg or MP4 format.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>   

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="https://dl.acm.org/journal/tomm" target="_blank">
                TOMM
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mon2021_tomm" class="col p-0">
                <h5 class="title mb-0">Task-independent Recognition of Communication Skills in Group Interaction Using Time-series Modeling.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Shogo Okada,</nobr>
                  and
                  <nobr>Yukiko I. Nakano.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    ACM Transactions on Multimedia Computing Communications and Applications, vol. 17, no. 4, pp. 122:1-122:27, 2021.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mon2021_tomm-abstract" role="button" aria-expanded="false" aria-controls="mon2021_tomm-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.1145/3450283" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://dl.acm.org/doi/pdf/10.1145/3450283" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mon2021_tomm-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Case studies of group discussions are considered an effective way to assess communication skills (CS). This method can help researchers evaluate participants’ engagement with each other in a specific realistic context. In this article, multimodal analysis was performed to estimate CS indices using a three-task-type group discussion dataset, the MATRICS corpus. The current research investigated the effectiveness of engaging both static and time-series modeling, especially in task-independent settings. This investigation aimed to understand three main points: first, the effectiveness of time-series modeling compared to nonsequential modeling; second, multimodal analysis in a task-independent setting; and third, important differences to consider when dealing with task-dependent and task-independent settings, specifically in terms of modalities and prediction models. Several modalities were extracted (e.g., acoustics, speaking turns, linguistic-related movement, dialog tags, head motions, and face feature sets) for inferring the CS indices as a regression task. Three predictive models, including support vector regression (SVR), long short-term memory (LSTM), and an enhanced time-series model (an LSTM model with a combination of static and time-series features), were taken into account in this study. Our evaluation was conducted by using the R2 score in a cross-validation scheme. The experimental results suggested that time-series modeling can improve the performance of multimodal analysis significantly in the task-dependent setting (with the best R2 = 0.797 for the total CS index), with word2vec being the most prominent feature. Unfortunately, highly context-related features did not fit well with the task-independent setting. Thus, we propose an enhanced LSTM model for dealing with task-independent settings, and we successfully obtained better performance with the enhanced model than with the conventional SVR and LSTM models (the best R2 = 0.602 for the total CS index). In other words, our study shows that a particular time-series modeling can outperform traditional nonsequential modeling for automatically estimating the CS indices of a participant in a group discussion with regard to task dependency.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://www.apsipa2021.org/" target="_blank">
                APSIPA
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mu2021_apsipa" class="col p-0">
                <h5 class="title mb-0">Improving Security in McAdams Coefficient-Based Speaker Anonymization by Watermarking Method.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Tokyo, Japan, December 2021.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mu2021_apsipa-abstract" role="button" aria-expanded="false" aria-controls="mu2021_apsipa-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/9689554" target="_blank">IEEE</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9689554&tag=1" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mu2021_apsipa-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Speaker anonymization aims to suppress speaker individuality to protect privacy in speech while preserving the other aspects, such as speech content. One effective solution for anonymization is to modify the McAdams coefficient. In this work, we propose a method to improve the security for speaker anonymization based on the McAdams coefficient by using a speech watermarking approach. The proposed method consists of two main processes: one for embedding and one for detection. In embedding process, two different McAdams coefficients represent binary bits "0" and "1". The watermarked speech is then obtained by frame-by-frame bit inverse switching. Subsequently, the detection process is carried out by a power spectrum comparison. We conducted objective evaluations with reference to the VoicePrivacy 2020 Challenge (VP2020) and of the speech watermarking with reference to the Information Hiding Challenge (IHC) and found that our method could satisfy the blind detection, inaudibility, and robustness requirements in watermarking. It also significantly improved the anonymization performance in comparison to the secondary baseline system in VP2020.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

      </ol>
    </div>
  </div>
  <!--  -->

  <!-- 2020 -->
  <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2020</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="http://www.apsipa.org/proceedings/2020/APSIPA-ASC-2020.html" target="_blank">
                APSIPA
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mwu2020_apsipa" class="col p-0">
                <h5 class="title mb-0">Speech Information Hiding by Modification of LSF Quantization Index in CELP Codec.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Shengbei Wang,</nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Auckland, New Zealand, December 2020.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mwu2020_apsipa-abstract" role="button" aria-expanded="false" aria-controls="mwu2020_apsipa-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/9306401" target="_blank">IEEE</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="http://www.apsipa.org/proceedings/2020/pdfs/0001321.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mwu2020_apsipa-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      A prospective method for securing digital speech communication is by hiding the information within the speech. Most of the speech information hiding methods proposed in prior research are lacking in robustness when dealing with the encoding process (e.g. the code-excited linear prediction (CELP) codec). The CELP codecs provide a codebook that represents the encoded signal at a lower bit rate. As essential features in speech coding, line spectral frequencies (LSFs) are generally included in the codebook. Consequently, LSFs are considered as a prospective medium for information hiding that is robust against CELP codecs. In this paper, we propose a speech information hiding method that modifies the least significant bit of the LSF quantization obtained by a CELP codec. We investigated the feasibility of our proposed method by objective evaluation in terms of detection accuracy and inaudibility. The evaluation results confirmed the reliability of our proposed method with some further potential improvement (multiple embedding and varying segmentation lengths). The results also showed that our proposed method is robust against several signal processing operations, such as resampling, adding Gaussian noise, and several CELP codecs (i.e., the Federation Standard-1016 CELP, G.711, and G.726).
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="http://www.interspeech2020.org/" target="_blank">
                Interspeech
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mgku2020_interspeech" class="col p-0">
                <h5 class="title mb-0">X-Vector Singular Value Modification and Statistical-Based Decomposition with Ensemble Regression Modeling for Speaker Anonymization System.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>,</nobr> <nobr>Kasorn Galajit,</nobr> <nobr>Jessada Karnjana, </nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, pp. 1703–1707, October 2020.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mgku2020_interspeech-abstract" role="button" aria-expanded="false" aria-controls="mgku2020_interspeech-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.isca-speech.org/archive/interspeech_2020/mawalim20_interspeech.html" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/mawalim20_interspeech.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mgku2020_interspeech-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Anonymizing speaker individuality is crucial for ensuring voice privacy protection. In this paper, we propose a speaker individuality anonymization system that uses singular value modification and statistical-based decomposition on an x-vector with ensemble regression modeling. An anonymization system requires speaker-to-speaker correspondence (each speaker corresponds to a pseudo-speaker), which may be possible by modifying significant x-vector elements. The significant elements were determined by singular value decomposition and variant analysis. Subsequently, the anonymization process was performed by an ensemble regression model trained using x-vector pools with clustering-based pseudo-targets. The results demonstrated that our proposed anonymization system effectively improves objective verifiability, especially in anonymized trials and anonymized enrollments setting, by preserving similar intelligibility scores with the baseline system introduced in the VoicePrivacy 2020 Challenge.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold success-color-dark align-middle" style="width: 75px;" href="https://www.springer.com/series/11156" target="_blank">
                Springer
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mu2020_springer" class="col p-0">
                <h5 class="title mb-0">Audio Information Hiding Based on Cochlear Delay Characteristics with Optimized Segment Selection.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em> 
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    Advances in Intelligent Systems and Computing, Springer, vol. 1145, 2020.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mu2020_springer-abstract" role="button" aria-expanded="false" aria-controls="mu2020_springer-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://link.springer.com/chapter/10.1007/978-3-030-46828-6_12" target="_blank">DOI</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mu2020_springer-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      Audio information hiding (AIH) based on cochlear delay (CD) characteristics is a promising technique to deal with the trade-off between inaudibility and robustness requirements effectively. However, the use of phase-shift keying (PSK) for blindly detectable AIH based on CD characteristics caused abrupt phase changing (phase spread spectrum), which leads to bad inaudibility. This paper proposed the technique to reduce the spread spectrum from PSK by segment selection process with spline interpolation optimization. Objective evaluation to measure the detection accuracy (BDR) and inaudibility (PEAQ and LSD) was carried out with 102 various genre music clips dataset. Based on the evaluation result, our proposed method could successfully reduce the spread spectrum caused by PSK by having improvement on inaudibility test with adequate detection accuracy up to 1024 bps.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>
      </ol>
    </div>
  </div>
  <!--  -->

  <!-- 2019 -->
  <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2019</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 75px;" href="http://www.risp.jp/" target="_blank">
                RISP
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mu2019_risp" class="col p-0">
                <h5 class="title mb-0">Feasibility of Audio Information Hiding Using Linear Time Variant IIR Filters Based on Cochlear Delay.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em></nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    Journal of Signal Processing, Research Institute of Signal Processing, vol. 23, no. 4, 2019.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mu2019_risp-abstract" role="button" aria-expanded="false" aria-controls="mu2019_risp-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://doi.org/10.2299/jsp.23.155" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.jstage.jst.go.jp/article/jsp/23/4/23_155/_pdf/-char/en" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mu2019_risp-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      A reported technique for cochlear delay (CD) based audio information hiding achieved imperceptibility in non-blind approaches. However, the phase shift keying (PSK) technique was utilized with a blind method, causing drastically phase changes that imply perceptible information was inserted. This paper presents an investigation on the feasibility of hiding information in the linear time variant (LTV) system. We adapted a CD filter design for the LTV system in the embedding scheme. The detection scheme was conducted using instantaneous chirp-z transformation (CZT). Objective tests for checking the imperceptibility (PEAQ and LSD) and for checking data payload (bit detection rate (BDR)) were conducted to evaluate our method. Our experimental results supported the feasibility of utilizing the CD-based audio information hiding in the LTV system. ln addition, detection regarding both imperceptibility and data payload was better in our method than in the previous blind method.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold success-color-dark align-middle" style="width: 75px;" href="https://www.springer.com/gp/computer-science/lncs" target="_blank">
                LNCS
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="monu2019_lncs" class="col p-0">
                <h5 class="title mb-0">Multimodal BigFive Personality Trait Analysis using Communication Skill Indices and Multiple Discussion Types Dataset.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>, </nobr> <nobr>Shogo Okada, </nobr><nobr>Yukiko I. Nakano, </nobr>
                  and
                  <nobr>Masashi Unoki.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    Springer LNCS Social Computing and Social Media: Design, Human Behavior, and Analytics, Springer, vol. 11578, 2019.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#monu2019_lncs-abstract" role="button" aria-expanded="false" aria-controls="monu2019_lncs-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://link.springer.com/chapter/10.1007/978-3-030-21902-4_27" target="_blank">DOI</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://link.springer.com/content/pdf/10.1007/978-3-030-21902-4_27.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="monu2019_lncs-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      This paper focuses on multimodal analysis in multiple discussion types dataset for estimating BigFive personality traits. The analysis was conducted to achieve two goals: First, clarifying the effectiveness of multimodal features and communication skill indices to predict the BigFive personality traits. Second, identifying the relationship among multimodal features, discussion type, and the BigFive personality traits. The MATRICS corpus, which contains of three discussion task types dataset, was utilized in this experiment. From this corpus, three sets of multimodal features (acoustic, head motion, and linguistic) and communication skill indices were extracted as the input for our binary classification system. The evaluation was conducted by using F1-score in 10-fold cross validation. The experimental results showed that the communication skill indices are important in estimating agreeableness trait. In addition, the scope and freedom of conversation affected the performance of personality traits estimator. The freer a discussion is, the better personality traits estimator can be obtained.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>
      </ol>
    </div>
  </div>
  <!--  -->

  <!-- 2017 -->
  <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2017</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://ieeexplore.ieee.org/xpl/conhome/8306063/proceeding" target="_blank">
                ICEEI
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mlp2017_iceei" class="col p-0">
                <h5 class="title mb-0">POS-based reordering rules for Indonesian-Korean statistical machine translation.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>, </nobr><nobr>Dessi Puji Lestari, </nobr>
                  and
                  <nobr>Ayu Purwarianti.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    6th International Conference on Electrical Engineering and Informatics (ICEEI), pp. 1-6, 2017.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mlp2017_iceei-abstract" role="button" aria-expanded="false" aria-controls="mlp2017_iceei-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/abstract/document/8312383" target="_blank">IEEE</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mlp2017_iceei-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      In SMT system, reordering problem is one of the most important and difficult problems to solve. The problem becomes definitely serious due to the different grammatical pattern between source and target language. The previous research about reordering model in SMT use the distortion-based reordering approach. However, this approach is not suitable for Indonesian-Korean translation. The main reason is because the word order between Indonesian and Korean are mostly reversed. Therefore, in this study, we develop a source-side reordering rules by using POS tag and word alignment information. This technique is promising to solve the reordering problem based on the experimental result. By applying 130 reordering rules in ID-KR and 50 reordering rules for KR-ID translation, the quality of translation in term of BLEU score increases 1.25% for ID-KR translation and 0.83% for KR-ID translation. Besides, combining this reordering rules with Korean verb formation rules for ID-KR translation can increase the BLEU score from 38.07 to 49.46 (in 50 simple sentences evaluation).
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="row m-0 mt-3 p-0">
            <div class="col-sm-1 p-0 abbr">
              <a class="badge font-weight-bold primary-color-dark align-middle" style="width: 75px;" href="https://aclanthology.org/volumes/Y17-1/" target="_blank">
                PACLIC
              </a>
            </div>
            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
              <div id="mlp2017_paclic" class="col p-0">
                <h5 class="title mb-0">Rule-based Reordering and Post-Processing for Indonesian-Korean Statistical Machine Translation.</h5>
                <div class="author">
                  <nobr><em>Candy Olivia Mawalim</em>, </nobr><nobr>Dessi Puji Lestari, </nobr>
                  and
                  <nobr>Ayu Purwarianti.</nobr>
                </div>
                <div>
                  <p class="periodical font-italic">
                    In Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation, pages 287–295, Cebu, Phillippines, 2017.
                  </p>
                </div>
                <div class="col p-0">
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mlp2017_paclic-abstract" role="button" aria-expanded="false" aria-controls="mlp2017_paclic-abstract">Abstract</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/Y17-1039/" target="_blank">ACL</a>
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/Y17-1039.pdf" target="_blank">PDF</a>
                </div>
                <div class="col mt-2 p-0">
                  <div id="mlp2017_paclic-abstract" class="collapse">
                    <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                      In SMT system, reordering problem is one of the most important and difficult problems to solve. The problem becomes definitely serious due to the different grammatical pattern between source and target language. The previous research about reordering model in SMT use the distortion-based reordering approach. However, this approach is not suitable for Indonesian-Korean translation. The main reason is because the word order between Indonesian and Korean are mostly reversed. Therefore, in this study, we develop a source-side reordering rules by using POS tag and word alignment information. This technique is promising to solve the reordering problem based on the experimental result. By applying 130 reordering rules in ID-KR and 50 reordering rules for KR-ID translation, the quality of translation in term of BLEU score increases 1.25% for ID-KR translation and 0.83% for KR-ID translation. Besides, combining this reordering rules with Korean verb formation rules for ID-KR translation can increase the BLEU score from 38.07 to 49.46 (in 50 simple sentences evaluation).
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </li>
      </ol>
    </div>
  </div>
  <!--  -->

</div>

  <!-- Footer -->
  <footer>
    &copy; Copyright 2022 Candy Olivia Mawalim. Powered by <a href="http://jekyllrb.com/" target="_blank"><font color="#a20d0d">Jekyll</font></a> with <a href="https://github.com/alshedivat/al-folio"><font color="#a20d0d">al-folio</font></a> theme. Hosted by <a href="https://pages.github.com/" target="_blank"><font color="#a20d0d">GitHub Pages</font></a>.
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-54519238-1', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
